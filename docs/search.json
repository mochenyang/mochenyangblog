[
  {
    "objectID": "posts/2024-07-10-Shrinkage/2024-07-10-Shrinkage.html",
    "href": "posts/2024-07-10-Shrinkage/2024-07-10-Shrinkage.html",
    "title": "Lasso vs. Ridge",
    "section": "",
    "text": "Lasso and Ridge regressions are two popular machine learning methods for both classification and numeric prediction tasks. They apply a shrinkage term on top of a standard multivariate linear regression that penalizes model complexity. Specifically, the shrinkage term for Lasso is the \\(L_1\\) norm of all parameters / coefficients, whereas it is the \\(L_2\\) norm for Ridge.\nInterestingly, if you have used Lasso and Ridge in practice, you will likely notice that Lasso is much more likely to penalize some coefficients to exactly 0 than Ridge. As such, Lasso is sometimes used as a feature selection mechanism. This blog post provides an intutive and graphical explanation about why this is the case. This post is inspired by the textbook The Elements of Statistical Learning by Hastie, Tibshirani, and Friedman."
  },
  {
    "objectID": "posts/2024-07-10-Shrinkage/2024-07-10-Shrinkage.html#an-optimization-formulation-of-lasso-and-ridge",
    "href": "posts/2024-07-10-Shrinkage/2024-07-10-Shrinkage.html#an-optimization-formulation-of-lasso-and-ridge",
    "title": "Lasso vs. Ridge",
    "section": "An Optimization Formulation of Lasso and Ridge",
    "text": "An Optimization Formulation of Lasso and Ridge\nFor concreteness and ease of visualization, let’s think about training a Lasso / Ridge regression model using two features \\(\\{X_1, X_2\\}\\) to predict a real-valued output \\(Y\\). Suppose we have a set of training data \\(\\{X_{1,i}, X_{2,i}, Y_i\\}_{i=1}^N\\). The learning problem of Lasso / Ridge can be formulated as a minimization problem with a regularization term:\n\\[\n\\min_{\\beta_1, \\beta_2} \\sum_{i=1}^N \\left(Y_i - X_{1,i} \\beta_1 - X_{2,i} \\beta_2 \\right)^2 + \\lambda \\cdot \\text{regularization}\n\\]\nwhere \\(\\text{regularization} = |\\beta_1| + |\\beta_2|\\) for Lasso and \\(\\text{regularization} = \\beta_1^2 + \\beta_2^2\\) for Ridge.\nWhile this is technically correct, it is not easy to see why Lasso regularization is more likely to reduce some coefficients exactly to 0. Instead, we will rephrase the learning problem as a contrained optimization problem:\n\\[\n\\text{     Lasso:     }\\min_{\\beta_1, \\beta_2} \\sum_{i=1}^N \\left(Y_i - X_{1,i} \\beta_1 - X_{2,i} \\beta_2 \\right)^2 \\text{     subject to     } |\\beta_1| + |\\beta_2| \\leq t\n\\]\nand\n\\[\n\\text{     Ridge:     }\\min_{\\beta_1, \\beta_2} \\sum_{i=1}^N \\left(Y_i - X_{1,i} \\beta_1 - X_{2,i} \\beta_2 \\right)^2 \\text{     subject to     } \\beta_1^2 + \\beta_2^2 \\leq t\n\\]\nJust like the \\(\\lambda\\) parameter, the \\(t\\) parameter controls the magnitude of the complexity penalty. The benefits of this contrained optimization formulation is that it is much easier to visualize."
  },
  {
    "objectID": "posts/2024-07-10-Shrinkage/2024-07-10-Shrinkage.html#graphical-explanation",
    "href": "posts/2024-07-10-Shrinkage/2024-07-10-Shrinkage.html#graphical-explanation",
    "title": "Lasso vs. Ridge",
    "section": "Graphical Explanation",
    "text": "Graphical Explanation\nThe first step to understand what the regularization term does is to visualize three terms: the residual sum of square (as the loss function) and the two constraints.\nFirst, \\(\\sum_{i=1}^N \\left(Y_i - X_{1,i} \\beta_1 - X_{2,i} \\beta_2 \\right)^2\\) might look intimidating, but viewed as a function of coefficients \\(\\beta_1\\) and \\(\\beta_2\\), it is nothing but a quadratic function\n\\[\n\\begin{align*}\n\\sum_{i=1}^N \\left(Y_i - X_{1,i} \\beta_1 - X_{2,i} \\beta_2 \\right)^2 =& \\sum_{i=1}^N X_{1,i}^2 \\beta_1^2 + \\sum_{i=1}^N X_{2,i}^2 \\beta_2^2 \\\\\n&+ 2\\sum_{i=1}^N X_{1,i}X_{2,i} \\beta_1 \\beta_2 - 2\\sum_{i=1}^N X_{1,i}Y_i \\beta_1 - 2\\sum_{i=1}^N X_{2,i}Y_i \\beta_2 \\\\\n&+ \\sum_{i=1}^N Y_i^2\n\\end{align*}\n\\]\nThis is nothing but an ellipse in 2-D!\nNext, \\(|\\beta_1| + |\\beta_2| \\leq t\\) is a square with side length \\(\\sqrt{2}t\\) centered around origin and \\(\\beta_1^2 + \\beta_2^2 \\leq t\\) is a circle with radius \\(\\sqrt{t}\\) centered around origin.\nMoreover, to represent the contrained optimization problem, simply let \\(L(\\beta_1, \\beta_2) = \\sum_{i=1}^N \\left(Y_i - X_{1,i} \\beta_1 - X_{2,i} \\beta_2 \\right)^2\\) be the value of the residual sum of square given a specific set of values for \\(\\beta_1, \\beta_2\\), then the contrained optimization problem is equivalent to finding the smallest \\(L(\\beta_1, \\beta_2)\\) value such that the ellipse intersect with the square (for Lasso) or the circle (for Ridge).\nThe following figure visualizes the idea:\n\n\n\nshrinkage\n\n\nBecause the Lasso shrinkage term takes a square shape with sharp end points and flat edges, it is much more likely for the ellipse to intersect at the end points (causing at least one of the parameters to be exactly 0). In contrast, the Ridge shrinkage term is a round circle, and the ellipse may intersect at any point on the circle. This general intuitive becomes much stronger in higher dimensions, when the 2-D square becomes a high-dimensional cube and 2-D circle becomes a high-dimensional sphere."
  },
  {
    "objectID": "posts/2023-06-13-Hierarchical-Clustering/2023-06-13-Hierarchical-Clustering.html",
    "href": "posts/2023-06-13-Hierarchical-Clustering/2023-06-13-Hierarchical-Clustering.html",
    "title": "Cluster Distance in Hierarchical Clustering",
    "section": "",
    "text": "Hierarchical clustering is one of the most commonly used algorithms to discover clusters from data. It works by iteratively merging smaller clusters that are closest to each other into bigger ones. A key ingredient to hierarchical clustering is a metric to quantify the distance between two clusters (which is different from the measure of distance between to individual data points). A number of such cluster distance metrics (a.k.a. linkage methods) are available, such as single linkage, complete linkage, average linkage, centroid distance, and Ward’s method (see this wiki page for many more).\nDuring the process of hierarchical clustering, as clusters are being merged, there is the need to compute the distance between the a newly merged cluster and all the other clusters (or data points), in order to find the next two clusters to merge. Of course, one could simply maintain the raw data and re-compute all pair-wise distances among all clusters every time after a merge happens. This is, however, computationally quite wasteful because, at the very least, pair-wise distances among clusters that are not newly merged do not need to be computed again.\nWith computational efficiency in mind, it turns out that for a large collection of linkage methods, you do not even need to maintain the raw data. Calculate the pair-wise distance among all individual data points once to get the distance matrix, and that’s all you need moving forward. In fact, the hclust function in R, for example, explicitly takes distance matrix (not raw data) as input.\nHow does this work? In particular, how can we update the distance matrix once two (smaller) clusters are merged, to properly reflect the cluster distance between the newly formed cluster and the rest of data? The goal of this blog is to answer this question, through which we will get to know the Lance–Williams Algorithm, and also clarify some (often omitted) technical details about the centroid and Ward’s methods."
  },
  {
    "objectID": "posts/2023-06-13-Hierarchical-Clustering/2023-06-13-Hierarchical-Clustering.html#illustration-for-single-complete-average-linkage",
    "href": "posts/2023-06-13-Hierarchical-Clustering/2023-06-13-Hierarchical-Clustering.html#illustration-for-single-complete-average-linkage",
    "title": "Cluster Distance in Hierarchical Clustering",
    "section": "Illustration for Single / Complete / Average Linkage",
    "text": "Illustration for Single / Complete / Average Linkage\nThe \\(\\alpha,\\beta,\\gamma\\) parameters values for single, complete, and average linkage methods are:\n\n\n\n\n\n\n\n\n\n\nLinkage Method\n\\(\\alpha_1\\)\n\\(\\alpha_2\\)\n\\(\\beta\\)\n\\(\\gamma\\)\n\n\n\n\nSingle Linkage\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(-1/2\\)\n\n\nComplete Linkage\n\\(1/2\\)\n\\(1/2\\)\n\\(0\\)\n\\(1/2\\)\n\n\nAverage Linkage\n\\(n_I/(n_I+n_J)\\)\n\\(n_J/(n_I+n_J)\\)\n\\(0\\)\n\\(0\\)\n\n\n\nLet’s start with the single linkage method to illustrate why this is true. Recall that single linkage uses the nearest neighbors between the two clusters as the cluster distance, i.e., \\(d(I,J) = \\min_{i \\in I, j \\in J} d(i,j)\\). The RHS of the Lance-Williams equation is\n\\[\n1/2 d(I,K) + 1/2 d(J,K) - 1/2 |d(I,K) - d(J,K)|\n\\]\nIf \\(d(I,K) &gt; d(J,K)\\), the above simplifies to \\(d(J,K)\\), and if \\(d(I,K) &lt; d(J,K)\\), it simplifies to \\(d(I,K)\\). In other words, the above expression is equivalent to \\(\\min\\{d(I,K), d(J,K)\\}\\), which, by definition of single linkage, is exactly \\(d(IJ, k)\\). The same derivation will show you that the parameter values for complete linkage are also correct.\nNow, for average linkage, we only need to notice that, by definition, \\(d(I,K) = \\frac{\\sum d(i,k)}{n_I \\cdot n_K}\\) and \\(d(J,K) = \\frac{\\sum d(j,k)}{n_J \\cdot n_K}\\). Simply plugging in the parameters will show you that it works as intended."
  },
  {
    "objectID": "posts/2023-06-13-Hierarchical-Clustering/2023-06-13-Hierarchical-Clustering.html#illustration-for-centroid-and-wards-method",
    "href": "posts/2023-06-13-Hierarchical-Clustering/2023-06-13-Hierarchical-Clustering.html#illustration-for-centroid-and-wards-method",
    "title": "Cluster Distance in Hierarchical Clustering",
    "section": "Illustration for Centroid and Ward’s Method",
    "text": "Illustration for Centroid and Ward’s Method\nThe more (nuanced) cases are methods like centroid and Ward’s, which relies on the concept of “centroid” (i.e., geometric mean of a cluster). Their parameter values are as follows (now far from obvious):\n\n\n\n\n\n\n\n\n\n\nLinkage Method\n\\(\\alpha_1\\)\n\\(\\alpha_2\\)\n\\(\\beta\\)\n\\(\\gamma\\)\n\n\n\n\nCentroid Distance\n\\(n_I/(n_I+n_J)\\)\n\\(n_J/(n_I+n_J)\\)\n\\(-n_I n_J/(n_I+n_J)^2\\)\n\\(0\\)\n\n\nWard’s Method\n\\((n_I+n_K)/(n_I+n_J+n_K)\\)\n\\((n_J+n_K)/(n_I+n_J+n_K)\\)\n\\(-n_K/(n_I+n_J+n_K)\\)\n\\(0\\)\n\n\n\nHere I will derive this result for the centroid distance (and similar derivation can be done for the Ward’s method). First, let \\(C_I\\), \\(C_J\\), and \\(C_K\\) denote the centroids of the three initial clusters – they are single points just like individual data. Note that, by definition of centroid, \\(C_{IJ} = \\frac{n_I c_I + n_J c_J }{n_I+n_J}\\).\nFor now, let’s assume a Squared Euclidean distance metric (the reason is not clear at all at this point, but stay with me for now), meaning that for any two data points with coordinates \\(x=(x_1, \\ldots, x_M)\\) and \\(y=(y_1, \\ldots, y_M)\\), we have \\(d(x,y) = \\sum_{m=1}^M (x_m - y_m)^2\\) where \\(m\\) indexes each one of the \\(M\\) features. A nice thing about this metric is that the squared difference on each feature is fully additive. So, we don’t need to carry around the summation over all features – we can just need to work (symbolically) with \\((x-y)^2\\).\nNext, plug in the parameter values for the RHS of Lance-William, we get:\n\\[\nn_I/(n_I+n_J) (C_I - C_K)^2 + n_J/(n_I+n_J) (C_J - C_K)^2 -n_I n_J/(n_I+n_J)^2 (C_I - C_J)^2\n\\]\nOpen up all the squares and re-arrange the terms, we will eventually see that it indeed equals \\(d(IJ,K)\\), which is\n\\[\n\\left(\\frac{n_I c_I + n_J c_J }{n_I+n_J} - C_K \\right)^2\n\\]\nHowever, the important thing to notice is that the above derivation only works when the underlying distance metric is Squared Euclidean. This is why, when using centroid or Ward’s method for cluster distance, one should always pick Euclidean distance as the metric to measure distance between data points (then the software implementations will square those distances when performing Lance-Williams, see this as and example of how Scipy does it). In fact, the notation of “centroid” only really make sense in a Euclidean space. Technically, you can still adopt centroid / Ward’s methods with non-Euclidean distance measures, but the price you pay is that the Lance-Williams recursive relationship (which makes computation much easier) would no longer hold, and you need to re-compute cluster distances from raw data after every merge."
  },
  {
    "objectID": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html",
    "href": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html",
    "title": "Training Word Embeddings",
    "section": "",
    "text": "This post walks through a small concrete example to illustrate the skip-gram (SG) architecture to train word embeddings. The continuous bag-of-words (CBOW) architecture can be understood by following a very similar procedure."
  },
  {
    "objectID": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#setup",
    "href": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#setup",
    "title": "Training Word Embeddings",
    "section": "Setup",
    "text": "Setup\nTo make the mathematical details easier to follow, we will stick with a very small example. Specifically, imagine the training corpus contains a single sentence “I love trucks”, and our goal is to learn 2-dimensional embedding (i.e., \\(D=2\\)) for each of the three words in vocabulary \\(V=(I,love,trucks)\\). In practice, the training corpus typically contains a very large amounts of texts (e.g., millions of sentences), the vocabulary size is usually tens of thousands, and the embedding dimension is usually a few hundreds."
  },
  {
    "objectID": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#skip-gram-architecture",
    "href": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#skip-gram-architecture",
    "title": "Training Word Embeddings",
    "section": "Skip-Gram Architecture",
    "text": "Skip-Gram Architecture\nThe skip-gram model trains word embeddings by using a given focal word to predict a context word (i.e., a word in the vicinity of the focal word). The architecture is a fully-connected, feed-forward neural network with three layers: an input layer, a “projection” layer, and an output layer. The “projection” layer is a bit different from a typical hidden layer, in the sense that it does not apply any non-linear activation. For concreteness, consider the following training example from our sentence: let’s pick “love” as the focal word and use it to predict “trucks”. In other word, “love” is the input and the desired output is “trucks”.1\nIn the skip-gram network, both the input and output layers have exactly \\(V\\) neurons. The input layer is simply one-hot encoding of the input, i.e., the input neurons take values 1/0, corresponding to each words in the vocabulary. The output layer has the standard softmax activation. Importantly, the projection layer contains exactly \\(D\\) neurons. Its purpose is to “project” each input word to the embedding space (with dimension \\(D\\)), and then to “project” the embedding representation to produce the output. Using the \\(love \\rightarrow trucks\\) training example, here is exactly what happens in the network:\n\nwhere \\(w_{ij}\\) denote the weights from input to projection, and \\(u_{ji}\\) denote the weights from projection to output. For conciseness, we can also write these weights as vectors: \\(\\vec{w_1} = (w_{11},w_{12})\\), \\(\\vec{w_2} = (w_{21},w_{22})\\), \\(\\vec{w_3} = (w_{31},w_{32})\\); and similarly \\(\\vec{u_1}=(u_{11},u_{21})\\), \\(\\vec{u_2}=(u_{12},u_{22})\\), \\(\\vec{u_3}=(u_{13},u_{23})\\).\nNow, let’s run through the network and compute the information that flows in and out of each layer:\n\n\n\n\n\n\n\n\nLayer\nInformation In\nInformation Out\n\n\n\n\nInput\nword “love”\nOne-hot encoding \\((0,1,0)\\)\n\n\nProjection (1st neuron)\n\\(0 \\cdot w_{11}+1\\cdot w_{21}+0\\cdot w_{31} = w_{21}\\)\n\\(w_{21}\\) (directly pass the information)\n\n\nProjection (2nd neuron)\n\\(0 \\cdot w_{12}+1\\cdot w_{22}+0\\cdot w_{32} = w_{22}\\)\n\\(w_{22}\\) (directly pass the information)\n\n\nOutput (1st neuron)\n\\(w_{21} \\cdot u_{11} + w_{22} \\cdot u_{21} = \\vec{w_2} \\boldsymbol{\\cdot} \\vec{u}_1\\)\n\\(\\frac{\\exp(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u_1})}{\\sum_{i=1}^3 \\exp(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u_i})}\\) (standard softmax activation)\n\n\nOutput (2nd neuron)\n\\(w_{21} \\cdot u_{12} + w_{22} \\cdot u_{22} = \\vec{w_2} \\boldsymbol{\\cdot} \\vec{u}_2\\)\n\\(\\frac{\\exp(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u_2})}{\\sum_{i=1}^3 \\exp(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u_i})}\\) (standard softmax activation)\n\n\nOutput (3rd neuron)\n\\(w_{21} \\cdot u_{13} + w_{22} \\cdot u_{23} = \\vec{w_2} \\boldsymbol{\\cdot} \\vec{u}_3\\)\n\\(\\frac{\\exp(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u_3})}{\\sum_{i=1}^3 \\exp(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u_i})}\\) (standard softmax activation)\n\n\n\nThen, comparing the softmax-transformed probabilities at the output layer with the label \\((0,0,1)\\), one can derive the loss value associated with this example.2 Via gradient descent, the goal is to learn the network weights.\nImportantly, the network weights are treated as the embeddings of words. For example, \\((w_{11},w_{12})\\) is the 2-dimensional embedding of the word “I”, because it projects the word “I” into a 2-dimensional space. Notice that, in the above example, each word has two embeddings, respectively from the weights connecting input to projection and from the weights connecting projection to output (e.g., \\(\\vec{w_1}\\) and \\(\\vec{u_1}\\) for word “I”). The skip-gram model uses the weights connecting input to project (i.e., \\(\\vec{w_i}\\)) as the word embedding, whereas \\(\\vec{u_i}\\) are sometimes referred to as “context word embeddings”. It is important to treat the two sets of embeddings as separate embeddings (rather than share those parameters). Otherwise, the dot product between the input / focal word and itself (in the above example, \\(\\vec{w_2} \\boldsymbol{\\cdot} \\vec{u}_2\\)) will tend to have a much higher value than other dot products, even though the same word does not usually appear in the context of the focal word (for more technical details, see this pre-print)."
  },
  {
    "objectID": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#continuous-bag-of-words-cbow",
    "href": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#continuous-bag-of-words-cbow",
    "title": "Training Word Embeddings",
    "section": "Continuous Bag-of-Words (CBOW)",
    "text": "Continuous Bag-of-Words (CBOW)\nThe CBOW architecture can be understood in a similar way, by walking through the above example (a good exercise for readers). The key different is that CBOW uses a collection of context words to predict a focal word (analogous to a “fill in the blank” language task)."
  },
  {
    "objectID": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#footnotes",
    "href": "posts/2022-07-12-Word-Embedding/2022-07-12-Word-Embedding.html#footnotes",
    "title": "Training Word Embeddings",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn other words, each training instance is a pair of focal and context words. Additional technical details such as negative sampling is ignored here for simplicity.↩︎\nOf course, when training with many examples in practice, the loss values associated with all training instances in a batch will be summed before the gradient is computed.↩︎"
  },
  {
    "objectID": "posts/2022-04-11-LSTM-Gradient/2022-04-11-LSTM-Gradient.html",
    "href": "posts/2022-04-11-LSTM-Gradient/2022-04-11-LSTM-Gradient.html",
    "title": "The Thing about LSTM and Exploding Gradients",
    "section": "",
    "text": "An important challenge in training recurrent neural networks (RNNs) is the vanishing / exploding gradient problem. Here is an (over-simplified) illustration of the problem. Suppose you have an RNN with \\(T\\) time steps, with an initial input \\(x\\) (i.e., \\(h_0 = x\\)) and weight \\(w\\). Assuming a linear activation function (again, for simplicity). The hidden states at time \\(t\\) will be: \\[\nh_t = w h_{t-1} = w^t h_0 = w^t x\n\\] Therefore, the derivative / gradient with respect to parameter \\(w\\) is \\(\\frac{d h_t}{dw}=t w^{t-1} x\\). The longer the time steps \\(t\\), the higher the exponent in the \\(w^{t-1}\\). As a result, for long sequences, the gradient vanishes even if \\(w\\) is slightly smaller than 1, and it explodes even if \\(w\\) is slightly greater than 1. This makes training RNN unstable.\nAt the root of this problem is the self-multiplication of weights across many time steps. The parameter sharing technique that enables RNNs to handle variable-length sequences is also the culprit of the vanishing / exploding gradient problem.\nThe LSTM architecture offers robustness against the vanishing gradient problem. To understand how, let’s first layout the key pieces of a LSTM cell;\n\nforget gate: \\(forget_t = sigmoid(X_t, h_{t-1}, \\Theta_{forget})\\);\ninput gate: \\(input_t = sigmoid(X_t, h_{t-1}, \\Theta_{input})\\);\noutput gate: \\(output_t = sigmoid(X_t, h_{t-1}, \\Theta_{output})\\);\nUpdate internal cell state: \\(C_t = forget_t \\cdot C_{t-1} + input_t \\cdot tahn(X_t, h_{t-1}, \\Theta)\\);\nProduce output: \\(h_t = output_t \\cdot tahn(C_t)\\).\n\nwhere \\(\\Theta_{(.)}\\) are all parameters that the network learns from data. The three “gates” can be conceptually thought of as “weights”, and the real “magic” of LSTM lies in the internal cell state. Notice that \\(C_t\\) is “auto-regressive”, in the sense that it depends on \\(C_{t-1}\\) through the time-varying forget gate weights. Having the forget gate weights close to 1 would allow \\(C_t\\) to “memorize” information from previous states. This is what mitigates the vanishing gradient problem.\nHowever, the LSTM architecture does not address the exploding gradient problem. This is because the self-multiplication problem still exists through other variables, such as \\(output_i\\). If we remove the internal cell state for a moment, the output \\(h_t = output_i\\) would be exactly the same as what you get in a regular RNN architecture, where self-multiplication of \\(\\Theta_{output}\\) again is a problem.\nFor more technical / mathematical discussions of this issue, I recommend the following this StackExchange Q&A and this blog post."
  },
  {
    "objectID": "posts/2022-02-10-Rejections/2022-02-10-Rejections.html",
    "href": "posts/2022-02-10-Rejections/2022-02-10-Rejections.html",
    "title": "A Tale of Four Rejections",
    "section": "",
    "text": "They say you can learn more from failures than from successes. But that doesn’t seem to be the case when it comes to publishing academic papers. While successes are often widely celebrated, failures are rarely publicized. In this blog post, I would take the less-traveled path and talk about what I have learned about academic publishing from four rejections of our ForestIV paper. The idea behind this paper was initially developed during my final year in the Ph.D. program, but it took x years to publish it in xyz. During this prolonged journey to publication, it was rejected by PNAS, Management Science (twice), and Journal of Machine Learning Research (after 3 rounds).\nTo help articulate my reflection, let me put academic papers into the following 2 by 2 matrix, based on the quality of idea discussed in the paper (i.e., how novel / new the idea is) and the quality of execution (i.e., how rigorous / meticulous is the execution).\n\nPapers in categories A and D are easy to judge - A-type papers should clearly be accepted and D-type papers should clearly be rejected. However, what happens to papers in categories B and C are not immediately clear and ultimately reflect how journals handle the tradeoff between idea and execution. Our ForestIV paper (in our biased opinion) falls into category B. While it has a very unique idea (the novelty / quality of which has been consistently acknowledged even during the review process), its execution is imperfect, due to the difficulty of theoretically analyzing data-driven machine learning models and procedures. The fact that it has been repeatedly rejected at respected outlets made me understand: good execution of an incremental idea will have an easier time in today’s academic publishing than imperfect execution of a novel idea.\nAlthough this may have been obvious to some, it was not to me. While trying to make sense of it, I came to the realization that the objective of the peer review process, paradoxically, is at odds with the goal of scientific research to promote novel ideas and to have others gradually build upon those ideas. The objective of reviewers is to scrutinize a paper and produce a report. Focusing on the execution allows a reviewer to write a comprehensive report that shows both expertise and effort, whereas focusing on the idea could be subjective and more distant from the technical trainings that many reviewers have received."
  },
  {
    "objectID": "posts/2022-01-05-Natural-Experiments/2022-01-05-Natural-Experiments.html",
    "href": "posts/2022-01-05-Natural-Experiments/2022-01-05-Natural-Experiments.html",
    "title": "A Collection of Natural Experiments and Research Opportunities",
    "section": "",
    "text": "This is a living document of natural experiments and associated research questions that one might be able to study with them. It is most recently updated on 01/05/2022."
  },
  {
    "objectID": "posts/2022-01-05-Natural-Experiments/2022-01-05-Natural-Experiments.html#removal-of-dislike-counts-on-youtube",
    "href": "posts/2022-01-05-Natural-Experiments/2022-01-05-Natural-Experiments.html#removal-of-dislike-counts-on-youtube",
    "title": "A Collection of Natural Experiments and Research Opportunities",
    "section": "Removal of Dislike Counts on YouTube",
    "text": "Removal of Dislike Counts on YouTube\nWhat happened: On Nov 10, 2021, YouTube removed the dislike counts videos. While the likes count will be public to viewers, the dislikes count will only be privately accessible to the content creator. The change was rolled out on YouTube during that day, creating a sharp policy change. Notably, the dislike button was not removed, i.e., viewers can still click dislike, but they cannot see the total number of dislikes of a video.\nWhat questions can be studied:\n\nHow does the removal of an engagement feature affect viewers’ engagement towards videos?\nHow does the (asymmetric) removal of negative quality signal affect content quality on the platform?\n…"
  },
  {
    "objectID": "posts/2022-01-05-Natural-Experiments/2022-01-05-Natural-Experiments.html#congress-cut-finance-to-superconducting-supercollider",
    "href": "posts/2022-01-05-Natural-Experiments/2022-01-05-Natural-Experiments.html#congress-cut-finance-to-superconducting-supercollider",
    "title": "A Collection of Natural Experiments and Research Opportunities",
    "section": "Congress Cut Finance to Superconducting Supercollider",
    "text": "Congress Cut Finance to Superconducting Supercollider\nWhat happened: On Oct 19, 1993, U.S. congress voted to cancel the financing for superconducting supercollider. By then, the project was being constructed in Texas. The cancellation resulted in a significant shock to the physics job market, as graduates were not able to find academic jobs. Many of them, with solid quantitative training, instead found jobs in the finance industry, thereby creating a sudden influx of high-quality talent to the industry.\nWhat questions can be studied:\n\nHow does the (arguably exogenous) influx of high-quality talent affect the finance industry?\n…"
  },
  {
    "objectID": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html",
    "href": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html",
    "title": "Multicollinearity or Omitted Variable Bias? Answers to a Seeming Conundrum",
    "section": "",
    "text": "In statistics or causal inference courses, students are usually taught a number of “threats” they need to watch out for when running regression analyses, such as multicollinearity, omitted variable bias, reverse causality, measurement error, selection bias, etc. They often form the impression that the presence of these threats will “bias” their results (more specifically, regression estimations). In this blog post, I discuss a seeming conundrum involving multicollinearity and omitted variable bias."
  },
  {
    "objectID": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#whats-the-problem",
    "href": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#whats-the-problem",
    "title": "Multicollinearity or Omitted Variable Bias? Answers to a Seeming Conundrum",
    "section": "What’s the Problem?",
    "text": "What’s the Problem?\nSuppose you want to explore the effect of \\(X\\) on \\(Y\\) by considering the following simple linear regression model (technically speaking, the population regression equation):\n\\[\nY=\\beta_0+ \\beta_1 X + \\varepsilon\n\\]\nFor expositional simplicity, here I only consider a single independent variable. Adding additional control variables does not fundamentally change the discussion. Now, suppose another variable, \\(Z\\), also has an effect on \\(Y\\). More importantly, \\(Z\\) is correlated with \\(X\\), and let’s assume that the degree of correlation is not trivially small to be ignored. The question is: should you include \\(Z\\) in the regression model or not? This seemingly innocent question leads to the following conundrum:\n\nIf you include \\(Z\\) in the regression, you have a multicollinearity problem, because \\(Corr(X,Z)&gt;0\\) and the correlation is non-trivial (by assumption);\nIf you don’t include \\(Z\\) in the regression, you have an omitted variable bias problem, because \\(Z\\), by definition a part of the error term, is correlated with \\(X\\). I.e., \\(Corr(\\varepsilon,X)\\neq 0\\)."
  },
  {
    "objectID": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#nature-of-the-problem",
    "href": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#nature-of-the-problem",
    "title": "Multicollinearity or Omitted Variable Bias? Answers to a Seeming Conundrum",
    "section": "Nature of the Problem",
    "text": "Nature of the Problem\nAt first sight, this seems like a real conundrum, and one is forced to choose between two evils. To resolve it, it is important to first realize that multicollinearity and omitted variable bias are two issues with very different nature. Thinking they would both “bias” the estimation results is in fact a misconception.\nThe nature of the omitted variable problem is that it indeed biases coefficient estimations. In this case, roughly speaking:\n\\[\n\\mathbb{E}(\\widehat{\\beta_1}) = \\frac{Cov(X,Y)}{Var(X)} = \\beta_1 + \\frac{Cov(X,\\varepsilon)}{Var(X)} \\neq \\beta_1\n\\]\nIn other words, estimation of the parameter of interest, \\(\\beta_1\\), does not equal to its true value in expectation. Put differently, even if you have the luxury of having multiple independent samples (on which you can obtain multiple estimations of \\(\\beta_1\\)), your averaged estimation of \\(\\beta_1\\) would still be wrong.\nIn comparison, multicollinearity actually does not bias your estimates, it inflates the standard error estimation. It means that your estimation of \\(\\beta_1\\), while being correct in expectation, is less precise given a fixed sample. Proving the unbiasedness of estimation under multicollinearity takes more work (specifically, relies on the Frisch–Waugh–Lovell theorem theorem that students typically only encounter in Ph.D. level econometrics course). Therefore, I will use numeric simulations to demonstrate this.\nSpecifically, I will simulate 5000 data points for the following regression specification:\n\\[\nY=1+2 \\times X + 0.5 \\times Z + \\varepsilon\n\\]\nwhere \\(X \\sim N(0,1)\\) is the key independent variable of interest; \\(\\varepsilon \\sim N(0,0.1^2)\\) is the (exogenous) model error term; and \\(Z=0.6 \\times X+N(0,0.5^2)\\) is the “confounder” variable. Simple calculation will show that \\(Corr(X,Z) \\approx 0.77\\), so if \\(Z\\) is included in the regression with \\(X\\), there is nontrivial multicollinearity. Meanwhile, because \\(Z\\) is indeed in the data generation process of \\(Y\\), not having it in the regression would lead to non-trivial omitted variable bias. The simulation will be repeated for 1000 times (as if I could draw 1000 independent samples, each having 5000 data points, from certain population). The replicable code is included as follows.\nset.seed(123456)\nN = 5000\nN_iter = 1000\n# Iterate through 1000 simulation runs\n# Store the coefficient estimate on X in a data frame\nresults = data.frame(beta_mc = rep(NA, N_iter), beta_ovb = rep(NA, N_iter))\nfor (i in 1:N_iter) {\n    # Simulate independent variable X\n    X = rnorm(N)\n    # Simulate model error term eps\n    eps = rnorm(N, sd = 0.1)\n    # Simulate Z. Corr(Z,X) is around 0.77\n    Z = 0.6*X + rnorm(N, sd = 0.5)\n    # Simulate dependent variable Y\n    Y = 1 + 2*X + 0.5*Z + eps\n\n    # Have Z in the regression (multicollinearity)\n    model_mc = lm(Y ~ X + Z)\n    # Do not have Z in the regression (omitted variable bias)\n    model_ovb = lm(Y ~ X)\n\n    # store the coefficient on X from the two models\n    results[i, 1] = coef(model_mc)[\"X\"]\n    results[i, 2] = coef(model_ovb)[\"X\"]\n}\nNext, the empirical distribution of coefficient estimates on \\(X\\) is plotted.\nlibrary(ggplot2)\nggplot(results, aes(x = beta_mc)) +\n  geom_density() + \n  labs(x = \"Coefficient on X\", title = \"Multicollinearity\") +\n  theme_bw()\n\nggplot(results, aes(x = beta_ovb)) +\n  geom_density() +\n  labs(x = \"Coefficient on X\", title = \"Omitted Variable Bias\") +\n  theme_bw()\n\nWe can see that: (1) when \\(Z\\) is included in the regression, coefficient of \\(X\\) remains unbiased (centered around 2) despite the multicollinearity issue; (2) when \\(Z\\) is excluded from the regression, coefficient of \\(X\\) is biased (centered around 2.3)."
  },
  {
    "objectID": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#so-whats-the-answer",
    "href": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#so-whats-the-answer",
    "title": "Multicollinearity or Omitted Variable Bias? Answers to a Seeming Conundrum",
    "section": "So What’s the Answer?",
    "text": "So What’s the Answer?\nThe simulation seems to suggest that we should always include \\(Z\\) in the regression, because doing so does not really lead to any bias but not doing so will bias the estimations. The conclusion, in my opinion, is more nuanced than that.\nIn a sense, I argue that this conundrum isn’t actually a true conundrum (hence the “seeming” conundrum in the article title), and its resolution lies in thinking about “is \\(Z\\) really a variable that affects \\(Y\\) in addition to \\(X\\)”?\n\nIf \\(Z\\) really is a determinant of \\(Y\\) in addition to \\(X\\), just like what we have simulated (i.e., \\(Z\\) is a part of the data generation process of \\(Y\\)), then omitting \\(Z\\) causes bias, and it’s probably worth including \\(Z\\) in the regression. This is especially true if one has a sufficiently large sample, which makes the variance-inflation effect of multicollinearity relatively tolerable (from a hypothesis testing point of view);\nHowever, if \\(Z\\) does not provide any meaningful information beyond \\(X\\), then it shouldn’t be in the regression. As an example, think about the case of perfect multicollinearity (i.e., \\(Z\\) is simply a linear function of \\(X\\) such as \\(Z = 0.6 \\times X\\)), then \\(Z\\) does not provide any extra information beyond \\(X\\) and should be excluded from the regression. Importantly, excluding \\(Z\\) does NOT cause any omitted variable issue because \\(Z\\) is not part of the regression to begin with.\nIn practice, since the data generation process of \\(Y\\) is almost always unknown (that’s why we are running regressions in the first place), it can be hard to decide precisely whether \\(Z\\) provides any information beyond \\(X\\). At the end of the day, the inclusion or exclusion of variables in regression models should be guided by theory (i.e., think about what \\(X\\) and \\(Z\\) each measures in the particular problem context that’s being analyzed), not purely by empirics / data."
  },
  {
    "objectID": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#concluding-remarks",
    "href": "posts/2021-12-20-MC-OVB/2021-12-20-MC-OVB.html#concluding-remarks",
    "title": "Multicollinearity or Omitted Variable Bias? Answers to a Seeming Conundrum",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIn this post, we start from a conundrum that seems difficult at first, but after understanding the nature of the problem, becomes less of a conundrum. At the end of the day, its resolution depends on a subjective judgement (i.e., whether the potential confounder \\(Z\\) should be considered a determinant of \\(Y\\) or not). This is not uncommon in causal inference and is worth highlighting in my opinion. Although causal inference (and econometrics, for that matter) is often taken to be a highly quantitative field with little room for subjectivity, many practical decisions that one has to make when carrying out actual analyses (e.g., model specification, variable inclusion / exclusion) need to rely on subjective judgement and theoretical argumentation."
  },
  {
    "objectID": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html",
    "href": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html",
    "title": "Bias, Discrimination, and Algorithmic Fairness",
    "section": "",
    "text": "This is a (outdated) document on recent literature concerning discrimination and fairness issues in decisions driven by machine learning algorithms. A more comprehensive paper on this issue can be found here: Integrating Behavioral, Economic, and Technical Insights to Address Algorithmic Bias: Challenges and Opportunities for IS Research, which is published in ACM Transactions on Management Information Systems (TMIS)."
  },
  {
    "objectID": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#understanding-fairness",
    "href": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#understanding-fairness",
    "title": "Bias, Discrimination, and Algorithmic Fairness",
    "section": "Understanding Fairness",
    "text": "Understanding Fairness\nVarious notions of fairness have been discussed in different domains. Different fairness definitions are not necessarily compatible with each other, in the sense that it may not be possible to simultaneously satisfy multiple notions of fairness in a single machine learning model.\n\nDefinition of Fairness\nA common notion of fairness distinguishes direct discrimination and indirect discrimination. Direct discrimination happens when a person is treated less favorably than another person in comparable situation on protected ground (Romei and Ruggieri 2013; Zliobaite 2015). Here, comparable situation means the two persons are otherwise similarly except on a protected attribute, such as gender, race, etc. In contrast, indirect discrimination happens when an “apparently neutral practice put persons of a protected ground at a particular disadvantage compared with other persons” (Zliobaite 2015).\nThe two main types of discrimination are often referred to by other terms under different contexts. Direct discrimination is also known as systematic discrimination or disparate treatment, and indirect discrimination is also known as structural discrimination or disparate outcome. The disparate treatment/outcome terminology is often used in legal settings (e.g., Barocas and Selbst 2016).\nMore operational definitions of fairness are available for specific machine learning tasks. Consider a binary classification task. The outcome/label represent an important (binary) decision (pos and neg). One of the features is protected (e.g., gender, race), and it separates the population into several non-overlapping groups (e.g., GroupA and GroupB). The classifier estimates the probability that a given instance belongs to pos based on its features. Two notions of fairness are often discussed (e.g., Kleinberg et al. 2016): calibration within group and balance.\nCalibration within group means that for both groups, among persons who are assigned probability p of being pos, there should be p fraction of them that actually belong to pos in expectation. In other words, a probability score should mean what it literally means (in a frequentist sense) regardless of group. A violation of calibration means decision-maker has incentive to interpret the classifier’s result differently for different groups, leading to disparate treatment.\nBalance is class-specific. Balance for pos class means that average probability assigned to people in GroupA who are actually pos should be equal to the average probability assigned to people in GroupB who are actually pos. Balance for neg can be analogously defined. Balance intuitively means the classifier is not disproportionally inaccurate towards people from one group than the other. In other words, condition on the actual label of a person, the chance of misclassification is independent of the group membership. A violation of balance means that, among people who have the same outcome/label, those in one group are treated less favorably (assigned different probabilities) than those in the other. Balance can be formulated equivalently in terms of error rates, under the term of equalized odds (Pleiss et al. 2017) or disparate mistreatment (Zafar et al. 2016). In statistical terms, balance for a class is a type of conditional independence. It means that condition on the true outcome, the predicted probability of an instance belong to that class is independent of its group membership.\nSome other fairness notions are available. Statistical Parity requires members from the two groups should receive the same probability of being pos. One should not confuse statistical parity with balance, as the former does not concern about the actual outcomes - it simply requires average predicted probability of pos to be equal for two groups.\nFairness notions are slightly different (but conceptually related) for numeric prediction or regression tasks. Calders et al. (2013) discuss two definitions. First, equal means requires the average predictions for people in the two groups should be equal. This is conceptually similar to statistical parity in classification. Second, balanced residuals requires the average residuals (errors) for people in the two groups should be equal. This is conceptually similar to balance in classification.\n\n\nRelationship among Different Fairness Definitions\nFirst, not all fairness notions are equally important in a given context. For example, when base rate (i.e., the actual proportion of pos in a population) differs in the two groups, statistical parity may not be feasible (Kleinberg et al., 2016; Pleiss et al., 2017). Another case against the requirement of statistical parity is discussed in Zliobaite et al. (2011) and Kamiran et al. (2013). Specifically, statistical disparity in the data (measured as the difference between pos probabilities received by members of the two groups) is not all discrimination. Part of the difference may be explainable by other attributes that reflect legitimate/natural/inherent differences between the two groups. They argue that statistical disparity only after conditioning on these attributes should be treated as actual discrimination (a.k.a conditional discrimination). In addition, statistical parity ensures fairness at the group level rather than individual level. Dwork et al. (2011) argue for a even stronger notion of individual fairness, where pairs of similar individuals are treated similarly.\nSecond, not all fairness notions are compatible with each other. Kleinberg et al. (2016) show that the three notions of fairness in binary classification, i.e., calibration within groups, balance for pos class, and balance for neg class cannot be achieved simultaneously, unless under one of two trivial cases: (1) perfect prediction, or (2) equal base rates in two groups. Such impossibility holds even approximately (i.e., approximate calibration and approximate balance cannot all be achieved unless under approximately trivial cases). Pleiss et al. (2017) extends their work and shows that, when base rates differ, calibration is compatible only with a substantially relaxed notion of balance, i.e., weighted sum of false positive and false negative rates is equal between the two groups, with at most one particular set of weights. These incompatibility findings indicates trade-offs among different fairness notions. Speicher et al. (2018) discuss the relationship between group-level fairness and individual-level fairness. They define a fairness index over a given set of predictions, which can be decomposed to the sum of between-group fairness and within-group fairness. They theoretically show that increasing between-group fairness (e.g., increase statistical parity) can come at a cost of decreasing within-group fairness.\n\n\nRelationship between Fairness and Predictive Performance\nIn general, a discrimination-aware prediction problem is formulated as a constrained optimization task, which aims to achieve highest accuracy possible, without violating fairness constraints. There is evidence suggesting trade-offs between fairness and predictive performance. Calders et al, (2009) considered the problem of building a binary classifier where the label is correlated with the protected attribute, and proved a trade-off between accuracy and level of dependency between predictions and the protected attribute. Corbett-Davies et al. (2017) demonstrates that maximizing predictive accuracy with a single threshold (that applies to both groups) typically violates fairness constraints. Conversely, fairness-preserving models with group-specific thresholds typically come at the cost of overall accuracy. In essence, the trade-off is again due to different base rates in the two groups. Hardt et al. (2016) proposed algorithms to determine group-specific thresholds that maximize predictive performance under balance constraints, and similarly demonstrated the trade-off between predictive performance and fairness. Speicher et al. (2018) showed that a classifier achieve optimal fairness (based on their definition of a fairness index) can have arbitrarily bad accuracy performance.\nImportantly, such trade-off does not mean that one needs to build inferior predictive models in order to achieve fairness goals. In particular, in Hardt et al. (2016), the classifier is still built to be as accurate as possible, and fairness goals are achieved by adjusting classification thresholds. In the same vein, Kleinberg et al. (2018a) proved that “an equity planner” with fairness goals should still build the same classifier as one would without fairness concerns, and adjust decision thresholds. Moreover, such a classifier should take into account the protected attribute (i.e., group identifier) in order to produce correct predicted probabilities.\nAnother interesting dynamic is that discrimination-aware classifiers may not always be fair on new, unseen data (similar to the over-fitting problem). Cotter et al. (2018) discuss this issue, using ideas from hyper-parameter tuning."
  },
  {
    "objectID": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#addressing-algorithmic-bias",
    "href": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#addressing-algorithmic-bias",
    "title": "Bias, Discrimination, and Algorithmic Fairness",
    "section": "Addressing Algorithmic Bias",
    "text": "Addressing Algorithmic Bias\n\nMeasurement and Detection\nBoth Zliobaite (2015) and Romei et al. (2013) surveyed relevant measures of fairness or discrimination. One may compare the number or proportion of instances in each group classified as certain class. Sometimes, the measure of discrimination is mandated by law. For instance, the four-fifths rule (Romei et al. 2013) in hiring context requires the job selection rate for the protected group is at least 80% that of the other group. It is a measure of disparate impact.\nIf fairness or discrimination is measured as the number or proportion of instances in each group classified to a certain class, then one can use standard statistical tests (e.g., two sample t-test) to check if there is systematic/statistically significant differences between groups. Zliobaite (2015) review a large number of such measures, and Pedreschi et al. (2012) discuss relationships among different measures.\nIn the particular context of machine learning, previous definitions of fairness offer straightforward measures of discrimination. For instance, the degree of balance of a binary classifier for the positive class can be measured as the difference between average probability assigned to people with positive class in the two groups. In addition, Pedreschi et al. (2009) developed several metrics to quantify the degree of discrimination in association rules (or IF-THEN decision rules in general). Two similar papers are Ruggieri et al. (2010ab), which also associate these discrimination metrics with legal concepts, such as affirmative action. See also Kamishima et al. (2012) for more discussions on measuring different types of discrimination in IF-THEN rules. Speicher et al. (2018) define a fairness index that can quantify the degree of fairness for any two prediction algorithms. Their definition is rooted in the inequality index literature in economics. Yang and Stoyanovich (2016) develop measures for rank-based prediction outputs to quantify/detect statistical disparity.\nSpecialized methods have been proposed to detect the existence and magnitude of discrimination in data. Adebayo and Kagal (2016) use the orthogonal projection method to create multiple versions of the original dataset, each one removes an attribute and makes the remaining attributes orthogonal to the removed attribute. Then, the model is deployed on each generated dataset, and the decrease in predictive performance measures the dependency between prediction and the removed attribute. Zhang and Neil (2016) treat this as an anomaly detection task, and develop subset scan algorithms to find subgroups that suffer from significant disparate mistreatment.\nDiscrimination has been detected in several real-world datasets and cases. For example, Kamiran et al. (2012) identified discrimination in criminal records where people from minority ethnic groups were assigned higher risk scores. Caliskan et al. (2017) detect and document a variety of implicit biases in natural language, as picked up by trained word embeddings. Chouldechova (2017) showed the existence of disparate impact using data from the COMPAS risk tool.\n\n\nPrevention/Mitigation\nTechniques to prevent/mitigate discrimination in machine learning can be put into three categories (Zliobaite 2015; Romei et al. 2013): (1) data pre-processing, (2) algorithm modification, and (3) model post-processing.\nData pre-processing tries to manipulate training data to get rid of discrimination embedded in the data.\n\nCalders et al, (2009) propose two methods of cleaning the training data: (1) flipping some labels, and (2) assign unique weight to each instance, with the objective of removing dependency between outcome labels and the protected attribute. The first approach of flipping training labels is also discussed in Kamiran and Calders (2009), and Kamiran and Calders (2012).\nHajian et al. (2011) discuss a data transformation method to remove discrimination learned in IF-THEN decision rules. The high-level idea is to manipulate the confidence scores of certain rules.\nZemel et al. (2013) propose to learn a set of intermediate representation of the original data (as a multinomial distribution) that achieves statistical parity, minimizes representation error, and maximizes predictive accuracy.\nMancuhan and Clifton (2014) build non-discriminatory Bayesian networks. Their algorithm depends on deleting the protected attribute from the network, as well as pre-processing the data to remove discriminatory instances.\nBolukbasi et al. (2016) discuss de-biasing technique to remove stereotypes in word embeddings learned from natural language.\nCelis et al. (2016) study the problem of not only removing bias in the training data, but also maintain its diversity, i.e., ensure the de-biased training data is still representative of the feature space.\nLum and Johndrow (2016) propose to de-bias the data by transform the entire feature space to be orthogonal to the protected attribute.\nBower et al. (2018) use a regression-based method to transform the (numeric) label so that the transformed label is independent of the protected attribute conditioning on other attributes. This addresses conditional discrimination.\n\nAlgorithm modification directly modifies machine learning algorithms to take into account fairness constraints. A general principle is that simply removing the protected attribute from training data is not enough to get rid of discrimination, because other correlated attributes can still bias the predictions. This problem is known as redlining.\n\nCalders and Verwer (2010) propose to modify naive Bayes model in three different ways: (i) change the conditional probability of a class given the protected attribute; (ii) train two separate naive Bayes classifiers, one for each group, using data only in each group; and (iii) try to estimate a “latent class” free from discrimination.\nKamiran et al. (2010) develop a discrimination-aware decision tree model, where the criteria to select best split takes into account not only homogeneity in labels but also heterogeneity in the protected attribute in the resulting leaves.\nKamishima et al. (2011) use regularization technique to mitigate discrimination in logistic regressions. The regularization term increases as the degree of statistical disparity becomes larger, and the model parameters are estimated under constraint of such regularization.\nDwork et al. (2011) formulate a linear program to optimize a loss function subject to individual-level fairness constraints. They define a distance score for pairs of individuals, and the outcome difference between a pair of individuals is bounded by their distance. A follow up work, Kim et al. (2018), relaxes the knowledge requirement on the distance metric.\nFeldman et al. (2014) specifically designed a method to remove disparate impact defined by the four-fifths rule, by formulating the machine learning problem as a constraint optimization task.\nFish et al. (2014) adapt AdaBoost algorithm to optimize simultaneously for accuracy and fairness measures.\nBechavod and Ligett (2017) address the disparate mistreatment notion of fairness by formulating the machine learning problem as a optimization over not only accuracy but also minimizing differences between false positive/negative rates across groups.\nBerk et al. (2017) apply regularization method to regression models. The key contribution of their paper is to propose new regularization terms that account for both individual and group fairness.\nGrgic-Hlaca et al. (2017) propose to build ensemble of classifiers to achieve fairness goals. Interestingly, they show that an ensemble of unfair classifiers can achieve fairness, and the ensemble approach mitigates the trade-off between fairness and predictive performance.\nDwork et al. (2017) develop a decoupling technique to train separate models using data only from each group, and then combine them in a way that still achieves between-group fairness.\nAgarwal et al. (2018) reduces the fairness problem in classification (in particular under the notions of statistical parity and equalized odds) to a cost-aware classification problem.\n\nModel post-processing changes how the predictions are made from a model in order to achieve fairness goals.\n\nKamiran et al. (2010) propose to re-label the instances in the leaf nodes of a decision tree, with the objective to minimize accuracy loss and reduce discrimination. The predictions on unseen data are made not based on majority rule with the re-labeled leaf nodes."
  },
  {
    "objectID": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#other-thoughts",
    "href": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#other-thoughts",
    "title": "Bias, Discrimination, and Algorithmic Fairness",
    "section": "Other Thoughts",
    "text": "Other Thoughts\n\nThe issue of algorithmic bias is closely related to the interpretability of algorithmic predictions. By making a prediction model more interpretable, there may be a better chance of detecting bias in the first place. Meanwhile, model interpretability affects users’ trust toward its predictions (Ribeiro et al. 2016). Regulations have also been put forth that create “right to explanation” and restrict predictive models for individual decision-making purposes (Goodman and Flaxman 2016).\nThe design of discrimination-aware predictive algorithms is only part of the design of a discrimination-aware decision-making tool, the latter of which needs to take into account various other technical and behavioral factors. Such a gap is discussed in Veale et al. (2018)"
  },
  {
    "objectID": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#references",
    "href": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#references",
    "title": "Bias, Discrimination, and Algorithmic Fairness",
    "section": "References",
    "text": "References\n\nAdebayo, J., & Kagal, L. (2016). Iterative Orthogonal Feature Projection for Diagnosing Bias in Black-Box Models, 37. Retrieved from http://arxiv.org/abs/1611.04967\nAgarwal, A., Beygelzimer, A., Dudík, M., Langford, J., & Wallach, H. (2018). A Reductions Approach to Fair Classification.\nBarocas, S., & Selbst, A. (2016). Big Data’s Disparate Impact. California Law Review, 104(1), 671–729.\nBechavod, Y., & Ligett, K. (2017). Penalizing Unfairness in Binary Classification. Retrieved from http://arxiv.org/abs/1707.00044\nBerk, R., Heidari, H., Jabbari, S., Joseph, M., Kearns, M., Morgenstern, J., … Roth, A. (2017). A Convex Framework for Fair Regression, 1–5. Retrieved from http://arxiv.org/abs/1706.02409\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., & Kalai, A. (2016). Debiasing Word Embedding, (Nips), 1–9.\nBower, A., Niss, L., Sun, Y., & Vargo, A. (2018). Debiasing representations by removing unwanted variation due to protected attributes. Retrieved from https://www.fatml.org/media/documents/debiasing_representations.pdf\nCalders, T., & Verwer, S. (2010). Three naive Bayes approaches for discrimination-free classification. Data Mining and Knowledge Discovery, 21(2), 277–292.\nCalders, T., Kamiran, F., & Pechenizkiy, M. (2009). Building classifiers with independency constraints. ICDM Workshops 2009 - IEEE International Conference on Data Mining, (December), 13–18.\nCalders, T., Karim, A., Kamiran, F., Ali, W., & Zhang, X. (2013). Controlling attribute effect in linear regression. Proceedings - IEEE International Conference on Data Mining, ICDM, (1), 71–80.\nCaliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science, 356(6334), 183–186.\nCelis, L. E., Deshpande, A., Kathuria, T., & Vishnoi, N. K. (2016). How to be Fair and Diverse? Retrieved from http://arxiv.org/abs/1610.07183\nChouldechova, A. (2017). Fair Prediction with Disparate Impact : A Study of Bias in Recidivism Prediction Instruments. Big Data, 5(2), 153–163.\nCorbett-Davies, S., Pierson, E., Feller, A., Goel, S., & Huq, A. (2017). Algorithmic decision making and the cost of fairness. arXiv\nCotter, A., Gupta, M., Jiang, H., Srebro, N., Sridharan, K., & Wang, S. (2018). Training Fairness-Constrained Classifiers to Generalize.\nCusters, B. (2013). Discrimination and Privacy in the information society. Discrimination and Privacy in the Information Society (Vol. 3).\nDwork, C., Hardt, M., Pitassi, T., Reingold, O., & Zemel, R. (2011). Fairness Through Awareness. arXiv\nDwork, C., Immorlica, N., Kalai, A. T., & Leiserson, M. (2017). Decoupled classifiers for fair and efficient machine learning.\nFeldman, M., Friedler, S., Moeller, J., Scheidegger, C., & Venkatasubramanian, S. (2014). Certifying and removing disparate impact. arXiv\nFish, B., Kun, J., & Lelkes, A. (2014). Fair Boosting : a Case Study.\nGoodman, B., & Flaxman, S. (2016). European Union regulations on algorithmic decision-making and a “right to explanation,” 1–9.\nGrgic-Hlaca, N., Zafar, M. B., Gummadi, K. P., & Weller, A. (2017). On Fairness, Diversity and Randomness in Algorithmic Decision Making.\nHajian, S., Domingo-Ferrer, J., & Martinez-Balleste, A. (2011). Discrimination prevention in data mining for intrusion and crime detection. 2011 IEEE Symposium on Computational Intelligence in Cyber Security, 47–54.\nHardt, M., Price, E., & Srebro, N. (2016). Equality of Opportunity in Supervised Learning, (Nips).\nKamiran, F., & Calders, T. (2009). Classifying without discriminating. 2009 2nd International Conference on Computer, Control and Communication, IC4 2009.\nKamiran, F., & Calders, T. (2012). Data preprocessing techniques for classification without discrimination. Knowledge and Information Systems (Vol. 33).\nKamiran, F., Calders, T., & Pechenizkiy, M. (2010). Discrimination aware decision tree learning. Proceedings - IEEE International Conference on Data Mining, ICDM, 869–874.\nKamiran, F., Karim, A., Verwer, S., & Goudriaan, H. (2012). Classifying socially sensitive data without discrimination: An analysis of a crime suspect dataset. In Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012 (pp. 370–377).\nKamiran, F., Žliobaite, I., & Calders, T. (2013). Quantifying explainable discrimination and removing illegal discrimination in automated decision making. Knowledge and Information Systems (Vol. 35).\nKamishima, T., Akaho, S., & Sakuma, J. (2011). Fairness-aware learning through regularization approach. Proceedings - IEEE International Conference on Data Mining, ICDM, 643–650.\nKamishima, T., Akaho, S., Asoh, H., & Sakuma, J. (2012). Considerations on fairness-aware data mining. Proceedings - 12th IEEE International Conference on Data Mining Workshops, ICDMW 2012, 378–385.\nKim, M. P., Reingold, O., & Rothblum, G. N. (2018). Fairness Through Computationally-Bounded Awareness. https://doi.org/10.1145/2090236.2090255\nKleinberg, J., & Raghavan, M. (2018b). Selection Problems in the Presence of Implicit Bias. arXiv\nKleinberg, J., Lakkaraju, H., Leskovec, J., Ludwig, J., & Mullainathan, S. (2017). Human decisions and machine predictions. The quarterly journal of economics, 133(1), 237-293.\nKleinberg, J., Ludwig, J., Mullainathan, S., & Rambachan, A. (2018a). Algorithmic Fairness. AEA Papers and Proceedings, 108, 22–27.\nKleinberg, J., Mullainathan, S., & Raghavan, M. (2016). Inherent Trade-Offs in the Fair Determination of Risk Scores. arXiv\nLum, K., & Johndrow, J. (2016). A statistical framework for fair predictive algorithms, 1–6. Retrieved from http://arxiv.org/abs/1610.08077\nMancuhan, K., & Clifton, C. (2014). Combating discrimination using Bayesian networks. Artificial Intelligence and Law, 22(2), 211–238.\nPedreschi, D., Ruggieri, S., & Turini, F. (2009). Measuring Discrimination in Socially-Sensitive Decision Records. Proceedings of the 2009 SIAM International Conference on Data Mining, 581–592.\nPedreschi, D., Ruggieri, S., & Turini, F. (2012). A study of top-k measures for discrimination discovery. Proceedings of the 27th Annual ACM Symposium on Applied Computing.\nPleiss, G., Raghavan, M., Wu, F., Kleinberg, J., & Weinberger, K. Q. (2017). On Fairness and Calibration. arXiv\nRibeiro, M. T., Singh, S., & Guestrin, C. (2016). “Why Should I Trust You?”: Explaining the Predictions of Any Classifier. KDD 2016.\nRomei, A., & Ruggieri, S. (2013). A multidisciplinary survey on discrimination analysis. Knowledge Engineering Review, 29(5), 582–638.\nRuggieri, S., Pedreschi, D., & Turini, F. (2010a). Integrating induction and deduction for finding evidence of discrimination. Artificial Intelligence and Law, 18(1), 1–43.\nRuggieri, S., Pedreschi, D., & Turini, F. (2010b). Data mining for discrimination discovery. ACM Transactions on Knowledge Discovery from Data, 4(2), 1–40.\nSpeicher, T., Heidari, H., Grgic-Hlaca, N., Gummadi, K. P., Singla, A., Weller, A., & Zafar, M. B. (2018, July). A Unified Approach to Quantifying Algorithmic Unfairness: Measuring Individual &Group Unfairness via Inequality Indices. In Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining (pp. 2239-2248). ACM.\nVeale, M., Van Kleek, M., & Binns, R. (2018). Fairness and Accountability Design Needs for Algorithmic Support in High-Stakes Public Sector Decision-Making. CHI Proceeding, 1–14.\nYang, K., & Stoyanovich, J. (2016). Measuring Fairness in Ranked Outputs.\nZafar, M. B., Valera, I., Rodriguez, M. G., & Gummadi, K. P. (2016). Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment.\nZemel, R. S., Wu, Y., Swersky, K., Pitassi, T., & Dwork, C. (2013). Learning Fair Representations. Proceedings of the 30th International Conference on Machine Learning, 28, 325–333.\nZhang, Z., & Neill, D. B. (2016). Identifying Significant Predictive Bias in Classifiers, (June), 1–5. Retrieved from http://arxiv.org/abs/1611.08292\nZliobaite, I. (2015). A survey on measuring indirect discrimination in machine learning. arXiv\nZliobaite, I. (2015). On the relation between accuracy and fairness in binary classification. arXiv\nZliobaite, I., Kamiran, F., & Calders, T. (2011). Handling conditional discrimination. Proceedings - IEEE International Conference on Data Mining, ICDM, (1), 992–1001."
  },
  {
    "objectID": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#other-resources",
    "href": "posts/2018-08-11-Algorithmic-Fairness/2018-08-11-Algorithmic-Fairness.html#other-resources",
    "title": "Bias, Discrimination, and Algorithmic Fairness",
    "section": "Other Resources",
    "text": "Other Resources\nEmergence of Intelligent Machines: a series of talks on algorithmic fairness, biases, interpretability, etc."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mochen’s Blog",
    "section": "",
    "text": "Lasso vs. Ridge\n\n\n\n\n\n\nmachine-learning\n\n\nexposition\n\n\n\nAn interesting question on shrinkage methods with a graphical explanation\n\n\n\n\n\nJul 10, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nCommon Identification Strategies and Program Evaluation\n\n\n\n\n\n\nexposition\n\n\ncausal-inference\n\n\n\nConnections between linear regression, IV, DID and ATE, LATE, ATT estimations\n\n\n\n\n\nDec 26, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nCluster Distance in Hierarchical Clustering\n\n\n\n\n\n\nexposition\n\n\nmachine-learning\n\n\n\nAn In-Depth Discussion of Different Linkage Methods and the Lance–Williams Algorithm\n\n\n\n\n\nJun 13, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nThe Assumptions We Take for Granted\n\n\n\n\n\n\ncausal-inference\n\n\nexposition\n\n\nmachine-learning\n\n\n\nTwo interesting questions about linear regression\n\n\n\n\n\nMar 8, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nTraining Word Embeddings\n\n\n\n\n\n\nmachine-learning\n\n\nexposition\n\n\n\nA step-by-step illustration of skip-gram and continuous bag-of-words models\n\n\n\n\n\nJul 12, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe Non-Collapsibility of Logistic Regressions\n\n\n\n\n\n\ncausal-inference\n\n\nexposition\n\n\n\nLogit regression is really weird and I had no idea\n\n\n\n\n\nMay 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nThe Thing about LSTM and Exploding Gradients\n\n\n\n\n\n\nmachine-learning\n\n\nexposition\n\n\n\nWhy LSTM addresses vanishing gradients but not exploding ones?\n\n\n\n\n\nApr 11, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nPDP, M-Plot, and ALE\n\n\n\n\n\n\nmachine-learning\n\n\nexposition\n\n\n\nA Brief Description of Three Popular Model-Agnostic Interpretation Methods\n\n\n\n\n\nMar 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nForestIV\n\n\n\n\n\n\nresearch\n\n\n\nDemonstration of the ForestIV Implementation in R\n\n\n\n\n\nJan 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nA Collection of Natural Experiments and Research Opportunities\n\n\n\n\n\n\nresearch\n\n\n\nWhat happened when and what can be done\n\n\n\n\n\nJan 5, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nRandom Stuff Cancels Out, Right?\n\n\n\n\n\n\ncausal-inference\n\n\nexposition\n\n\n\nThe Perils of Random Measurement Error in Regression Analysis\n\n\n\n\n\nDec 27, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nMulticollinearity or Omitted Variable Bias? Answers to a Seeming Conundrum\n\n\n\n\n\n\ncausal-inference\n\n\nexposition\n\n\n\n\n\n\n\n\n\nDec 20, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nAn Intuitive Explanation of ROC and AUC\n\n\n\n\n\n\nmachine-learning\n\n\nexposition\n\n\n\n\n\n\n\n\n\nNov 23, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nBias, Discrimination, and Algorithmic Fairness\n\n\n\n\n\n\nmachine-learning\n\n\nresearch\n\n\n\n\n\n\n\n\n\nAug 11, 2018\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html",
    "href": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html",
    "title": "An Intuitive Explanation of ROC and AUC",
    "section": "",
    "text": "For students who are learning the basics of Machine Learning, one of the most challenging topics is the ROC curve and the AUC measure of a classifier. While students may be able to mechanically remember how to draw the ROC curve, they often do not fully understand why it is done that way. Similarly, while students know that AUC is the area under the ROC curve, it is not easy to understand why AUC measures the “the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one” (Wikipedia). The goal of this post is to provide an intuitive explanation for the two “why” questions. More specifically, I want to"
  },
  {
    "objectID": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#a-concrete-example",
    "href": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#a-concrete-example",
    "title": "An Intuitive Explanation of ROC and AUC",
    "section": "A Concrete Example",
    "text": "A Concrete Example\nFor concreteness, let’s consider a binary classification problem where the outcome is either Positive (\\(P\\)) or Negative (\\(N\\)). Suppose a classifier has made the following predictions (more specifically, the predicted probabilities of being in the \\(P\\) class) on a validation dataset of 10 records:\n\n\n\nID\nActual Class Label\nPredicted Probability of \\(P\\)\n\n\n\n\n1\n\\(P\\)\n0.99\n\n\n2\n\\(P\\)\n0.98\n\n\n3\n\\(N\\)\n0.96\n\n\n4\n\\(N\\)\n0.90\n\n\n5\n\\(P\\)\n0.88\n\n\n6\n\\(N\\)\n0.87\n\n\n7\n\\(P\\)\n0.85\n\n\n8\n\\(P\\)\n0.80\n\n\n9\n\\(N\\)\n0.70\n\n\n10\n\\(P\\)\n0.65\n\n\n\nNote that this is meant to be just an example – there’s nothing inherently special about the values of actual outcomes or predicted probabilities, and the intuitions explained later are generally applicable."
  },
  {
    "objectID": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#the-roc-curve",
    "href": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#the-roc-curve",
    "title": "An Intuitive Explanation of ROC and AUC",
    "section": "The ROC Curve",
    "text": "The ROC Curve\nThe ROC curve of a classifier lives on a two-dimensional plot, typically with False Positive Rate (FPR) as the \\(x\\)-axis and True Positive Rate (TPR) as the \\(y\\)-axis. If you are more familiar with the precision / recall terminologies, the False Positive Rate is equivalent to \\(1-Recall_N\\) and the True Positive Rate is equivalent to \\(Recall_P\\).\nMechanically, the ROC curve is constructed by calculating pairs of (FPR, TPR) for different prediction cutoff values, and plot them on a graph. It is convenient to think about a process where you start with a very high cutoff value, gradually reduce it, and calculate / plot (FPR, TPR) along the way. This is also why the table above has been ordered by the predicted probabilities (from high to low). So, let’s do this step-by-step for the above concrete example (and the ROC curve is provided at the end).\n\n[Step 1] If you pick a cutoff value higher than 0.99, no record will be predicted as \\(P\\) and all 10 records will be predicted as \\(N\\). This results in a TPR = 0 and FPR = 0;\n[Step 2] If you pick a cutoff value between 0.99 and 0.98, record 1 will be predicted as \\(P\\) and records 2-10 will be predicted as \\(N\\). This results in TPR = 1/6 and FPR = 0 (because record 1 is in \\(P\\) and your classifier correctly predicts that);\n[Step 3] If you pick a cutoff value between 0.98 and 0.96, records 1-2 will be predicted as \\(P\\) and records 3-10 will be predicted as \\(N\\). This results in TPR = 2/6 and FPR = 0;\n[Step 4] If you pick a cutoff value between 0.96 and 0.90, records 1-3 will be predicted as \\(P\\) and records 4-10 will be predicted as \\(N\\). This results in TPR = 2/6 and FPR = 1/4 (because record 3 is in fact \\(N\\), which means your classifier has 1 false positive prediction);\n…\n[Step 10] If you pick a cutoff value between 0.70 and 0.65, records 1-9 will be predicted as \\(P\\) and record 10 will be predicted as \\(N\\). This results in TPR = 5/6 and FPR = 1;\n[Step 11] If you pick a cutoff value below 0.65, all records will be predicted as \\(P\\) and no record will be predicted as \\(N\\). This results in TPR = 1 and FPR = 1.\n\n\nHaving walked through the above process of drawing the ROC curve, you may realize two important things:\n\nIt doesn’t really matter which specific cutoff values you choose. Essentially, you are going through the validation dataset from top to bottom (ranked based on predicted probability of being in \\(P\\)), one record at a time.\nAs you travel from top to bottom, looking at the actual outcome labels: if you hit a \\(P\\), it means that the classifier would produce a true positive prediction, and the ROC curve would go up; if you hit an \\(N\\), it means that the classifier would produce a false positive prediction, and the ROC curve would go right.\n\nThe second point is, in my opinion, a very useful intuition to have about the ROC curve, because it offers an intuitive understanding of what the ROC curve is visualizing \\(\\rightarrow\\) the performance of a classifier in terms of its ability to rank positive records ahead of negative ones (based on predicted probabilities of being positive). Why? Because the more positive records ranked ahead of negative ones, the more your ROC curve can go up without going right (i.e., the curve would occupy more space in the upper-left corner)."
  },
  {
    "objectID": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#the-auc-measure",
    "href": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#the-auc-measure",
    "title": "An Intuitive Explanation of ROC and AUC",
    "section": "The AUC Measure",
    "text": "The AUC Measure\nBased on the above intuition of the ROC curve, we can already derive a qualitative interpretation of the area under the ROC curve, or AUC. A larger AUC indicates that the ROC curve is able to go up for more steps before it needs to go right, which means that more positive records are ranked ahead of negative ones.\nIt turns out that AUC also has a very nice quantitative interpretation, i.e., the probability that a classifier will rank a randomly chosen positive instance higher than a randomly chosen negative one. However, it is not immediately clear why the area under the ROC curve maps nicely into this very particular probability. Below, I offer a visual “proof” of this fact.\nTo begin, it would be easier if we scale the \\(x\\)-axis and \\(y\\)-axis of the ROC curve by the number of negative and positive records. Doing so doesn’t change the ROC curve at all - it simply converts all the tick marks on the two axes into integers for convenience. In particular, suppose there are \\(M_P\\) number of positive records and \\(M_N\\) number of negative records, then the marks on the \\(x\\)-axis are scaled from \\(\\{0,\\frac{1}{M_N},\\frac{2}{M_N},\\ldots, 1\\}\\) into \\(\\{0,1,2,\\ldots, M_N\\}\\), and the marks on the \\(y\\)-axis are scaled from \\(\\{0,\\frac{1}{M_P},\\frac{2}{M_P},\\ldots, 1\\}\\) into \\(\\{0,1,2,\\ldots, M_P\\}\\). In our example, \\(M_P=6\\) and \\(M_N=4\\) and the scaled ROC curve looks like the following. After scaling, the ROC space is sliced up into 24 (\\(=4 \\times 6\\)) cells, each having an area of 1 - this layout will make it convenient to describe the visual proof.\n\nHere comes the visual proof. First, think about record #1, which is in class \\(P\\), that causes the ROC curve to jump up from \\((0,0)\\) to \\((0,1)\\). Notice that it has four cells, shaded in blue, to its right. If you recall the intuition we have developed from drawing the ROC curve, this observation means that there are precisely 4 records in class \\(N\\) that rank lower than this \\(P\\) record. Why? Because the ROC curve needs to move right 4 times before it hits the right boundary. Put differently, the area of the blue region (which is 4) represents the number of ways to pick a record in class \\(N\\) such that it ranks lower than this \\(P\\) record.\nMore importantly, the same way of thinking applies to each \\(P\\) record. Take the third record in \\(P\\) (which is record #5 in our table) as an example, it has 2 cells to its right, shaded in green. That simply means there are 2 ways to pick a record in class \\(N\\) such that it ranks lower than this \\(P\\) record.\nAs a result, the area under the ROC curve, which is the same as the area of all the shaded cells, amounts to the total number of ways to pick a record in class \\(N\\) such that it ranks lower than each particular \\(P\\) record. Now, keep in mind that the area of the entire ROC space (i.e., 24) is simply the total number of ways to pick a pair of \\(P\\) record and \\(N\\) record, you will see that the de-scaled AUC (i.e., area of the shaded region divided by the total area) would then represent the probability of choosing a pair of \\(P\\) record and \\(N\\) record and having the \\(P\\) record rank higher than the \\(N\\) record."
  },
  {
    "objectID": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#concluding-remarks",
    "href": "posts/2021-11-23-ROC-AUC/2021-11-23-ROC-AUC.html#concluding-remarks",
    "title": "An Intuitive Explanation of ROC and AUC",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nBy walking through a concrete example, I hope to have articulated the intuitions behind the ROC curve and the AUC measure. To conclude, I want to share three remarks that appear interesting to me personally.\n\nAlthough students rarely need to draw the ROC curve by hand (which can be highly cumbersome for large datasets), doing so for a tiny example is still useful for building up the intuition. The game-like procedure, where the curve moves up when you hit \\(P\\) and moves right when you hit \\(N\\), nicely reflects why the ROC curve captures a classifier’s performance in ranking data based on predicted probabilities.\nFor students who are familiar with calculus, it might be tempting to try to prove the probabilistic interpretation of AUC by doing (Riemann-style) integration. While it is certainly feasible, I find it more intuitive to think about it not as the area under the curve, but instead as the area to the right of the curve (in other words, think about a Lebesgue-style integration). Mathematically, they are equivalent, but the latter seems to be easier to wrap one’s head around.\n(Updated 1/28/2023) The above walkthrough also provide an intuitive way to understand why, for a binary classifier, AUC of class \\(P\\) always equals AUC of class \\(N\\). Suppose now the class of interest is \\(N\\) (rather than \\(P\\)), then two things will change: (1) we should rank the validation dataset based on predicted probability of \\(N\\), which, by definition, reverses the ranking by \\(P\\); and (2) a record with class \\(N\\) is now considered correct prediction, so we also switch going up with going right. The net result of these two changes is that the ROC curve of class \\(N\\) looks like that of class \\(P\\), only in reverse (i.e., start from \\((1,1)\\) and traveling backwards to \\((0,0)\\)). The area under the curve therefore does not change."
  },
  {
    "objectID": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html",
    "href": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html",
    "title": "Random Stuff Cancels Out, Right?",
    "section": "",
    "text": "Imagine you want to understand the impact of \\(X\\) on \\(Y\\) (and potentially controlling for other factors \\(\\boldsymbol{Z}\\)), and proceed to collect data on \\(\\{X,\\boldsymbol{Z},Y\\}\\) and run a linear regression of \\(Y\\) on \\(\\{X,\\boldsymbol{Z}\\}\\). The problem is, however, you cannot get a precise measurement of \\(X\\) (e.g., perhaps your measurement instrument is flawed), and your measurement \\(\\widehat{X}\\) is only an imperfect representation of true \\(X\\), in the following sense:\n\\[\n\\widehat{X} = X + e\n\\]\nIn other words, the imperfect measurement differs from the true measurement by a value \\(e\\), commonly known as the measurement error. Now, assume that this measurement error \\(e\\) is completely random, i.e., it is independent of anything in \\(\\{X,\\boldsymbol{Z},Y\\}\\). The question is: does the presence of such purely random measurement error cause any problem to your regression estimations?\nThrough teaching and research, I found that people (ranging from master students, Ph.D. students, all the way to professors) have a very strong belief that the answer to the above question is NO, i.e., having purely random measurement error doesn’t bias your regression estimations. And the intuition is simply that “random stuff cancels out”. Because the measurement error is completely random, it has equal chance of being larger or smaller than the true value, and on average, voila, things should be fine. This strong belief, in my opinion, makes the issue of random measurement error particular dangerous in regression analysis."
  },
  {
    "objectID": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html#random-measurement-error-biases-regression-estimations",
    "href": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html#random-measurement-error-biases-regression-estimations",
    "title": "Random Stuff Cancels Out, Right?",
    "section": "Random Measurement Error Biases Regression Estimations",
    "text": "Random Measurement Error Biases Regression Estimations\nRandom stuff don’t always cancels out, and yes, random measurement error does bias regression estimations. The proof can be found in most introductory econometrics textbooks, and is reproduced here.\nFirst, one needs to clearly understand the difference between a population regression equation and the regression that is being estimated (the difference is important in this case). While the population regression equation describes the true relationship between independent and dependent variables (i.e., the data generation process), the regression being estimated approximates such relationship based on the available measurements. In our setup, the population regression equation is\n\\[\nY = \\beta_0 + \\beta_1 X + \\boldsymbol{\\beta_2 Z} + \\varepsilon\n\\]\nwhere \\(\\beta_1\\) is the coefficient of interest. However, because only \\(\\widehat{X}\\) is available, the regression being estimated is \\(Y \\sim \\{\\widehat{X}, \\boldsymbol{Z}\\}\\). It follows that:\n\\[\nY = \\beta_0 + \\beta_1 (\\widehat{X}-e) + \\boldsymbol{\\beta_2 Z} + \\varepsilon = \\beta_0 + \\beta_1 \\widehat{X} + \\boldsymbol{\\beta_2 Z} + (\\varepsilon - \\beta_1 e)\n\\]\nand because \\(Cov(\\widehat{X}, \\varepsilon-\\beta_1 e) = Cov(X + e, \\varepsilon-\\beta_1 e) = -\\beta_1 Var(e) \\neq 0\\), the estimated regression essentially has an “omitted variable” bias, where the random measurement error \\(e\\) is left in the regression error term, and is correlated with the independent variable \\(\\widehat{X}\\). To paraphrase, although the measurement error is purely random, it is nevertheless not independent of itself.\nIn the case of simple linear regression, one can even derive the degree of bias:\n\\[\n\\mathbb{E}(\\widehat{\\beta_1}) = \\frac{Cov(Y,\\widehat{X})}{Var(\\widehat{X})} = \\beta_1 \\frac{Var(X)}{Var(X) + Var(e)}\n\\]\nAnd because \\(\\beta_1 \\frac{Var(X)}{Var(X) + Var(e)} &lt; \\beta_1\\), the bias manifests as attenuation, i.e., the coefficient of interest is underestimated."
  },
  {
    "objectID": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html#underestimation-is-ok-right",
    "href": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html#underestimation-is-ok-right",
    "title": "Random Stuff Cancels Out, Right?",
    "section": "Underestimation is OK, Right?",
    "text": "Underestimation is OK, Right?\nOften times, after I explain the above to someone, they may continue to insist that having random measurement error is no big deal, because it only results in underestimation of coefficient and hence makes your conclusions more “conservative” than they really are. This line of argument is equally dangerous, because of at least two reasons:\n\nBeyond the simple linear regression model discussed here, it is typically not true. With most generalized linear models (e.g., logit, probit, Poisson), overestimation is possible. Even with a simple linear regression, slightly more complex measurement error structure can result in overestimation. Our research paper demonstrate this point with both theoretical and empirical analyses.\nEven if the coefficient is underestimated (and therefore conservative), it could be a very bad outcome depending on the context. Suppose you are estimating the impact of COVID infection on mortality, underestimating the effect is arguably worse than overestimating it."
  },
  {
    "objectID": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html#concluding-remarks",
    "href": "posts/2021-12-27-Random-Me/2021-12-27-Random-Me.html#concluding-remarks",
    "title": "Random Stuff Cancels Out, Right?",
    "section": "Concluding Remarks",
    "text": "Concluding Remarks\nIntuitions can be powerful, but they are not always correct. The case of purely random measurement error is an example of how intuition alone can fail you in dangerous ways. Random stuff don’t always cancels out, and even random measurement error in independent covariates can cause biases to regression estimations."
  },
  {
    "objectID": "posts/2022-01-10-ForestIV/2022-01-10-ForestIV.html",
    "href": "posts/2022-01-10-ForestIV/2022-01-10-ForestIV.html",
    "title": "ForestIV",
    "section": "",
    "text": "This blog post describes the ForestIV R package, an implementation of the ForestIV approach to correct for estimation biases caused by having machine-learning-generated covariates in regression analyses.\nTo install the package, simply run the following command in R:\nThe package depends on three external libraries:"
  },
  {
    "objectID": "posts/2022-01-10-ForestIV/2022-01-10-ForestIV.html#forestiv-implementation",
    "href": "posts/2022-01-10-ForestIV/2022-01-10-ForestIV.html#forestiv-implementation",
    "title": "ForestIV",
    "section": "ForestIV Implementation",
    "text": "ForestIV Implementation\nThe core function of the package is ForestIV(), which takes the following inputs:\n\ndata_test: A dataframe of the testing data (in a training-testing split) when building the random forest. It must have a column named “actual” that contains the ground truth values, and separate columns that contain each tree’s predictions named “X1”, “X2”, … Note that if you use the randomForest package to train the random forest model, then the column names will be taken care of automatically.\ndata_unlabel: A dataframe of the unlabeled data. It must have separate columns that contain each tree’s predictions named “X1”, “X2”, …\ncontrol: A character vector of control variable names. Pass an empty vector if there is no control variable.\nmethod: The method for IV selection. Supported values are “Lasso” for lasso-based selection and “IIV” for the imperfect IV method (forthcoming in our next paper, stay tuned!).\niterative: Whether or not to perform iterative IV selection to remove invalid and weak IVs, default to TRUE. This parameter is only relevant when method = “Lasso”.\nntree: Number of trees in the random forest.\nmodel_unbias: A lm or glm object that contains the unbiased regression estimates, typically obtained by running the regression on the labeled data.\nfamily: Model specification. Same as the family parameter in glm.\ndiagnostic: Whether to output diagnostic correlations for instrument validity and strength. Default to TRUE, which will produce four additional columns in the output, respectively named “pp_abs_before”, “pe_abs_before”, “pp_abs_after” and “pe_abs_after”. The “pp_abs_*” columns contain the average absolute correlation between endogenous covariate and IVs, before and after IV selection. Similarly, the “pe_abs_*” columns contain the average absolute correlation between the model error term and the IVs, before and after IV selection. If IV selection works properly, one should expect “pp_abs_after” to be higher than “pp_abs_before” (indicating that strong IVs are being selected) and “pe_abs_after” to be lower than “pe_abs_before” (indicating that invalid IVs are being removed).\n\nThe output is a dataframe with no more than ntree number of rows, each corresponding to the ForestIV estimation results where a specific tree is used as the endogenous covariate (and other trees as candidate IVs)."
  },
  {
    "objectID": "posts/2022-01-10-ForestIV/2022-01-10-ForestIV.html#a-replicable-demonstration",
    "href": "posts/2022-01-10-ForestIV/2022-01-10-ForestIV.html#a-replicable-demonstration",
    "title": "ForestIV",
    "section": "A Replicable Demonstration",
    "text": "A Replicable Demonstration\nFor demonstration, I replicate a part of the simulations reported in our paper, using the Bike Sharing Dataset, which contains 17,379 records of hourly bike rental activities. The machine learning task is to predict (the log of) rental counts based on time, seasonal, and weather features. The following code prepares the dataset.\n# Replicable Simulation Example with Bike Sharing Dataset\nlibrary(ForestIV)\nlibrary(dplyr)\nlibrary(randomForest)\n\n# import Bike Sharing data\n# removed \"instant\" (ID), \"dteday\" (date), and \"registered\"/\"casual\" (which simply add up to the outcome variable)\nBike = read.csv(\"hour.csv\", stringsAsFactors = FALSE) %&gt;%\n  dplyr::mutate(lnCnt = log(cnt)) %&gt;%\n  dplyr::select(-instant, -dteday, -registered, -casual, -cnt)\nNext, let’s train the random forest model. I use 1,000 data points for training, reserve 200 for testing, and the rest 16,379 are treated as unlabeled. After the model is trained, I obtain its predictions both on the testing data and on the unlabeled data.\n# parameters for random forest\nntree = 100\nN = nrow(Bike)\nN_train = 1000\nN_test = 200\nN_unlabel = N - N_train - N_test\n\n# need to set the same seed here to fully replicate this demonstration\nset.seed(123456)\n# train random forest\ntrain = sample(1:nrow(Bike), N_train)\ntest = sample((1:nrow(Bike))[-train], N_test)\nunlabel = sample((1:nrow(Bike))[-c(train, test)], N_unlabel)\nBike.rf=randomForest(lnCnt ~ . , data = Bike,\n                     mtry = 3, subset = train, ntree = ntree)\n\n# retrieve ground truth and predictions on testing and unlabeled data\n# setting \"predict.all = TRUE\" will produce predictions from each individual tree in the forest\nactual = Bike$lnCnt\npred_unlabel = predict(Bike.rf, Bike[unlabel,], predict.all = TRUE)\nindiv_pred_unlabel = pred_unlabel$individual\naggr_pred_unlabel = pred_unlabel$aggregate\npred_test = predict(Bike.rf, Bike[test,], predict.all = TRUE)\nindiv_pred_test = pred_test$individual\naggr_pred_test = pred_test$aggregate\nNext, let’s simulate the second-stage regression, where the ML predictions enter as an independent covariate with measurement error. Same as what we have done in the paper, I simulate a simple linear regression: \\(Y=1+0.5\\times lnCnt + 2 \\times control_1 + control_2 + \\varepsilon\\), where \\(lnCnt\\) is the ground truth values of log rental count, and \\(\\{control_1, control_2\\}\\) are two exogenous control variables that follow \\(Uniform[-10,10]\\) and \\(N(0,10^2)\\) respectively. The regression model error term, \\(\\varepsilon\\), follows \\(N(0,2^2)\\).\n# simulate data for econometric model\ncontrol1 = runif(N, min = -10, max = 10)\ncontrol2 = rnorm(N, sd = 10)\nepsilon = rnorm(N, sd = 2)\ncontrol = c(\"control1\", \"control2\")\nY = 1.0 + 0.5*actual + 2.0*control1 + control2 + epsilon\n\n# prepare various data partitions for estimations\ndata_train = data.frame(Y = Y[train], control1 = control1[train], control2 = control2[train], actual = actual[train])\ndata_test = data.frame(indiv_pred_test, aggr_pred_test, actual = actual[test])\ndata_label = data.frame(Y = Y[c(train, test)], control1 = control1[c(train, test)], control2 = control2[c(train, test)], actual = actual[c(train, test)])\ndata_unlabel = data.frame(Y = Y[unlabel], control1 = control1[unlabel], control2 = control2[unlabel], indiv_pred_unlabel, aggr_pred_unlabel)\nNow, let’s estimate the biased regression using the unlabeled data (with predicted \\(lnCnt\\) as covariate) and the unbiased regression using the labeled data. In the biased regression, coefficient on ML-generated covariate is overestimated! In the unbiased regression, coefficient estimates are fine, but the standard errors are expectedly larger (due to smaller sample size).\n# biased regression\nmodel_biased = lm(Y ~ aggr_pred_unlabel + control1 + control2, data = data_unlabel)\nsummary(model_biased)\n#Results:\n#                  Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)       0.637370   0.062325   10.23   &lt;2e-16 ***\n#aggr_pred_unlabel 0.580574   0.013315   43.60   &lt;2e-16 ***\n#control1          1.998247   0.002752  726.19   &lt;2e-16 ***\n#control2          0.998151   0.001602  623.10   &lt;2e-16 ***\n\n# unbiased regression\nmodel_unbias = lm(Y ~ actual + control1 + control2, data = data_label)\nsummary(model_unbias)\n#Results:\n#            Estimate Std. Error t value Pr(&gt;|t|)    \n#(Intercept)  1.05690    0.18596   5.683 1.66e-08 ***\n#actual       0.49197    0.03882  12.672  &lt; 2e-16 ***\n#control1     2.00336    0.01009 198.565  &lt; 2e-16 ***\n#control2     1.00356    0.00586 171.270  &lt; 2e-16 ***\nFinally, let’s run ForestIV estimation.\n# ForestIV estimation\nresult = ForestIV(data_test = data_test, data_unlabel = data_unlabel, control = control,\n                  method = \"Lasso\", iterative = TRUE, ntree = ntree, model_unbias = model_unbias,\n                  family = gaussian(link = \"identity\"), diagnostic = TRUE)\nThe result is a dataframe with 99 rows (because the procedure failed to find valid and strong IVs for one tree in the random forest). It can be further processed to produce the final estimations. Same as what we have done in the paper, here I report the ForestIV estimates that are statistically closest to the unbiased estimates (in the Mean-Squared-Error sense) and also pass the Hotelling test at the 95% significance level.\nH_critical = qchisq(0.95, df = 4)\ncoef_unbiased = coef(model_unbias)\nresult %&gt;% \n  mutate(bias2 = sum((c(beta_1, beta_2, beta_3, beta_4) - coef_unbiased)^2),\n         variance = se_1^2 + se_2^2 + se_3^2 + se_4^2,\n         mse = bias2+variance) %&gt;%\n  arrange(mse) %&gt;%\n  filter(row_number() == 1 & Hotelling &lt; H_critical)\n#Results:\n#beta_1 beta_2  beta_3  beta_4\n#0.984  0.503   1.998   0.998\nTo obtain valid standard error estimates, one can bootstrap the above procedure multiple times. As we show in the paper, the (bootstrapped) standard errors of ForestIV estimates are smaller than those in the unbiased regression. In other words, ForestIV produce less biased point estimates (compared to the biased regression) with higher precision (compared to the unbiased regression)."
  },
  {
    "objectID": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html",
    "href": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html",
    "title": "PDP, M-Plot, and ALE",
    "section": "",
    "text": "This is an exposition of three techniques, namely Partial Dependence Plot (PDP), Marginal Plot (M-Plot), and Accumulated Local Effects (ALE) Plot, which are popular model-agnostic methods to measure and visualize the “effect” of a given feature on the predictions of a ML model. A key reference for this post is the research article that proposed ALE. Notations are also largely borrowed from this article."
  },
  {
    "objectID": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html#partial-dependence-plot-pdp",
    "href": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html#partial-dependence-plot-pdp",
    "title": "PDP, M-Plot, and ALE",
    "section": "Partial Dependence Plot (PDP)",
    "text": "Partial Dependence Plot (PDP)\nPDP represents a straightforward (yet somewhat naïve) approach to quantify the effect of a feature on predictions. Specifically, for a given feature value \\(x_1\\), it simply computes the average prediction when \\(X_1 = x_1\\), and the “average” is taken over all possible values of \\(X_2\\), in order to “marginalize” the effect of the second feature on predictions. More precisely, PDP computes:\n\\[\nf_{1,PDP}(x_1) = \\mathbb{E}(f(x_1, X_2)) = \\int p_2(x_2) f(x_1, x_2) dx_2\n\\]\nthen plots \\(f_{1,PDP}(x_1)\\) against \\(x_1\\) for different values of \\(x_1\\).\nDespite its intuitiveness, PDP suffers from two key limitations:\n\nBecause the average prediction at \\(X_1 = x_1\\) is taken over all possible values of \\(X_2\\) (i.e., taken over the entire density of \\(X_2\\)), it is committing “extrapolation error”, namely that it includes \\((x_1,x_2)\\) that does not actually exist in the data into the computation.\nIt also relies on the “feature independence assumption”. If \\(X_1\\) and \\(X_2\\) are not independent (e.g., correlated), then the computation in Eq (1) is generally “biased”, in the sense that it blends the effects of both features."
  },
  {
    "objectID": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html#marginal-plot-m-plot",
    "href": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html#marginal-plot-m-plot",
    "title": "PDP, M-Plot, and ALE",
    "section": "Marginal Plot (M-Plot)",
    "text": "Marginal Plot (M-Plot)\nM-Plot builds on PDP by making a single tweak. For a given feature value \\(x_1\\), it still computes the average prediction when \\(X_1 = x_1\\), but now the “average” is taken over values of \\(X_2\\) conditional on \\(X_1 = x_1\\). More precisely, M-plot computes:\n\\[\nf_{1,M}(x_1) = \\mathbb{E}(f(X_1, X_2)|X_1 = x_1) = \\int p_{2|1}(x_2|x_1) f(x_1, x_2) dx_2\n\\]\nthen plots \\(f_{1,M}(x_1)\\) against \\(x_1\\) for different values of \\(x_1\\).\nBy taking the average over the conditional density of \\(X_2\\) on \\(X_1=x_1\\), it effectively avoids the “extrapolation error”, because only feasible values of \\(X_2\\) when \\(X_1 = x_1\\) are considered in the computation. Moreover, if the two features are indeed independent, then Eq (2) will become equivalent to Eq (1). However, it turns out that the validity of M-Plot still relies on the “feature independence assumption” (more on this later)."
  },
  {
    "objectID": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html#accumulated-local-effects-ale-plot",
    "href": "posts/2022-03-23-PDP-M-ALE/2022-03-23-PDP-M-ALE.html#accumulated-local-effects-ale-plot",
    "title": "PDP, M-Plot, and ALE",
    "section": "Accumulated Local Effects (ALE) Plot",
    "text": "Accumulated Local Effects (ALE) Plot\nFinally, the ALE was proposed as an interpretation method that remains robust when \\(X_1\\) and \\(X_2\\) are not independent. It computes the “marginal effect” of \\(X_1\\), then accumulates such marginal effects to obtain the overall main effect. More precisely, ALE computes:\n\\[\nf_{1,ALE}(x_1) = \\int_{\\min(X1)}^{x_1} \\mathbb{E}(f^1 (X_1, X_2)|X_1 = z_1) dz_1 = \\int_{\\min(X1)}^{x_1} \\int p_{2|1}(x_2|z_1) f^1(z_1, x_2) dx_2 dz_1\n\\]\nwhere \\(f^1(z_1, x_2)\\) is the partial derivative of prediction w.r.t. the first feature, i.e., \\(f^1(z_1, x_2)=\\frac{\\partial f(z_1,x_2)}{\\partial z_1}\\), which measures the “marginal effect” of \\(X_1\\) on the predictions in the neighborhood of \\(X_1 = z_1\\). Then, such marginal effects are accumulated from the minimum value that \\(X_1\\) takes, up to the focal value \\(x_1\\). Finally, \\(f_{1,ALE}(x_1)\\) is plotted against \\(x_1\\) for different values of \\(x_1\\) to get the ALE plot.\nThe elements of differentiation and accumulation in ALE are not super intuitive at first sight. For example, it is not immediately clear (1) why ALE addresses the feature dependency issue (and why M-plot doesn’t); and (2) why the integration in ALE starts from \\(\\min(X_1)\\), i.e., the minimum value of feature \\(X_1\\). In the next section, I offer an intuitive (though not 100% rigorous) explanation for both questions."
  },
  {
    "objectID": "posts/2022-05-24-Logit-Regression/2022-05-24-Logit-Regression.html",
    "href": "posts/2022-05-24-Logit-Regression/2022-05-24-Logit-Regression.html",
    "title": "The Non-Collapsibility of Logistic Regressions",
    "section": "",
    "text": "Acknowledgement: most of the content in this post come from discussions with Gord Burtch.\n\nOnce in a while, I learn about a fact so mind-blowing that it fundamentally changes how I view certain things. Such moments give me a mixed feeling of anxiety and joy: anxious about all the mistakes I may have made because of my ignorance but happy about the fact that I get to fix that ignorance to some degree. This is one of those moments. It has to do with how one should interpret (and more broadly, think about) coefficient estimates in a logistic regression.\n\nCollapsibility of Linear Regressions\nBefore we get to logit regression, let’s first take a detour to the nice-and-sunny land of linear regressions. Consider the following data generation process: \\[\nY=1+X_1+0.5X_2, \\text{where } X_1 \\sim N(1,2^2) \\text{ and } X_2 \\sim Unif(-2,2) \\text{ and } X_1 \\bot X_2\n\\] In other words, a dependent variable \\(Y\\) is jointly determined by a normal variable \\(X_1\\) and a uniform variable \\(X_2\\), and the two covariates are independent of each other. Simple enough. Now, suppose we want to use linear regression to estimate this relationship, based on a sample of \\(N=10000\\) data points.\nIn almost all practical statistical inference tasks, we cannot claim to know everything about the data generation process, and there may always be factors that we are not considering in a given regression. For concreteness, let’s say that \\(X_2\\) is not included in the regression. Using linear regression terminology, we can say that \\(X_2\\) is (a part of) the model’s error term. Because \\(X_1\\) and \\(X_2\\) are independent of each other, omitting \\(X_2\\) does not pose any serious problem to the estimation, except for making the standard errors of coefficients a bit larger. Our estimation of the coefficient on \\(X_1\\) (the included variable) will still be unbiased.\nThis is also easy to see with a simple simulation:\n# simulate data\nset.seed(123456)\nN = 10000\nx1 = rnorm(N, mean = 1, sd = 2)\nx2 = runif(N, min = -2, max = 2)\nxb = 1 + x1 + 0.5*x2\ny_linear = xb\n\n# linear regression, omitting x2\nsummary(lm(y_linear ~ x1))\n# we get a coefficient on x1 of 1.001 and standard error of 0.003\n# you can also repeat this multiple times to get the distribution of coefficient and see that it's unbiased.\nThis property, formally known as the collapsibility and informally means “independent unobservables are not a problem”, feels so natural that I have never truly questioned it. In fact, I cannot imagine living without it. As one can never hope to account for all factors driving a dependent variable, the collapsibility property is necessary to convince ourselves that our estimates are still somewhat useful, by arguing that whatever we leave out of the regression is independent from the things included in the regression. It also enables us to interpret a given coefficient with confidence, saying something like “holding everything else constant, 1 unit change in \\(X_1\\) is associated with 1 unit change in \\(Y\\)”.\n\n\nNon-Collapsibility of Logit Regressions\nNow, the nightmare is that logistic regressions, a tool so commonly used to model binary outcomes, are non-collapsible. Let’s continue with the same simulation setup to see this:\n# run the previous block to obtain variables\ny_logit = rbinom(N, 1, prob = 1/(1+exp(-xb)))\n\n# logit regression, having both x1 and x2\nsummary(glm(y_logit ~ x1 + x2, family = \"binomial\"))\n# As expected, coefficient estimates are quite close to their true values. After all, we are running the regression with the true data-generation process\n\n# logit regression, omitting x2\nsummary(glm(y_logit ~ x1, family = \"binomial\"))\n# coefficient on x1 is 0.933 with standard error of 0.022. The true value (1.0) is more than 3 SDs away...\n\n# Perhaps we got unlucky once? Replicating 1000 times to get distribution\nset.seed(123456)\nest = data.frame(b1_full = rep(NA, N), b1_omit = rep(NA, N))\nfor (i in 1:1000) {\n  x1 = rnorm(N, mean = 1, sd = 2)\n  x2 = runif(N, min = -2, max = 2)\n  xb = 1 + x1 + 0.5*x2\n  y_logit = rbinom(N, 1, prob = 1/(1+exp(-xb)))\n  model_full = glm(y_logit ~ x1 + x2, family = \"binomial\")\n  model_omit = glm(y_logit ~ x1, family = \"binomial\")\n  est[i,1] = coef(model_full)[2]\n  est[i,2] = coef(model_omit)[2]\n}\n\n# Let's plot it\nlibrary(ggplot2)\nlibrary(gridExtra)\np1 = ggplot(est, aes(x = b1_full)) + \n  geom_density() + \n  geom_vline(xintercept = 1.0, color = \"red\") + \n  labs(x = \"Coefficient on X1\", title = \"Distribution with {X1, X2} Included\") + \n  theme_bw()\n\np2 = ggplot(est, aes(x = b1_omit)) + \n  geom_density() + \n  geom_vline(xintercept = 1.0, color = \"red\") + \n  labs(x = \"Coefficient on X1\", title = \"Distribution with X2 Omitted\") + \n  theme_bw()\n\ngrid.arrange(p1, p2, nrow = 1)\n\nThe red vertical line marks where the true coefficient value is. We can see that omitting \\(X_2\\) result in biased estimate of the coefficient on \\(X_1\\), despite the fact that \\(X_2\\) is independent from \\(X_1\\)!!! As surprising as this is to me, it is actually widely known in fields such as epidemiology (e.g., see this paper and this blog post).\nSo what should we do if we still want to use logistic regression for statistical inference? Short answer is that any coefficient can only be interpreted as a “conditional effect”, condition on the other covariates that you choose to include in the regression. Change the covariates included, you may also change the coefficient estimate of any focal covariate, even if they are all independent. This blog post has some additional recommendations."
  },
  {
    "objectID": "posts/2023-03-08-LR-Assumptions/2023-03-08-LR-Assumptions.html",
    "href": "posts/2023-03-08-LR-Assumptions/2023-03-08-LR-Assumptions.html",
    "title": "The Assumptions We Take for Granted",
    "section": "",
    "text": "Acknowledgement: content in this post come from discussions with Xuan Bi. Question 1 below is adapted from an interview question brought to my attention by Yuxuan Yang (MSBA Class of 2023).\n\nIn this blog post, I will discuss two interesting questions related to linear regression, one focusing on the coefficient estimation aspect (e.g., for statistical inference) and the other focusing on the prediction aspect (e.g., as a numeric prediction model). A central theme underlying the answers to both questions is an assumption behind linear regression that is often taken for granted.\n\nQuestion 1: Estimating Regression with Replicating Data\nYou have a sample of \\(N\\) data points \\((x_i,y_i)_{i=1}^N\\), with which you want to estimate the following linear regression model\n\\[\nY=\\beta x + \\varepsilon\n\\]\nThe intercept term is omitted for simplicity (e.g., imagine the data has been centered). For this question, suppose that none of the “usual suspects” that might threaten the estimation exist, i.e., there is no endogeneity whatsoever. Denote the coefficient estimate as \\(\\widehat{\\beta}^{(N)}\\). Now imagine you make a copy of every single data point, thereby creating a sample of \\(2N\\) data points. Then estimate the same regression on this twice-as-large sample and denote the coefficient estimate as \\(\\widehat{\\beta_1}^{(2N)}\\). The question is: what is the relationship between \\(\\widehat{\\beta_1}^{(N)}\\) and \\(\\widehat{\\beta}^{(2N)}\\), and between \\(SE(\\widehat{\\beta}^{(N)})\\) and \\(SE(\\widehat{\\beta}^{(2N)})\\)?\nTo answer this question, one could (mechanically) follow the least-square formula for coefficient estimate and its standard error and discover that\n\\[\n\\widehat{\\beta}^{(2N)} = \\widehat{\\beta}^{(N)}, ~~~ SE(\\widehat{\\beta}^{(2N)})=\\frac{1}{\\sqrt{2}}SE(\\widehat{\\beta}^{(N)})\n\\]\nIn other words, estimating a regression on twice the data still gives the same coefficients but smaller standard errors. The same conclusion can be easily verified via a simulation.\nHowever, if you think a bit deeper, this answer is really weird. Just by making a copy of every single data point in a sample, we are able to “magically” reduce the standard error of coefficient estimates. Put differently, without bringing in any new information, we can somehow make our estimates more precise. This cannot be possible - taking this logic to its extreme, we can simply make multiple copies of every data point and achieve arbitrarily precise estimates from essentially finite data.\nWhat has gone wrong? The answer is hidden in an assumption of linear regression that is often taken for granted, namely the i.i.d assumption. The i.i.d assumption states that the data points in the sample to estimate regression (1) are assumed to be independently and identically distributed. This assumption is clearly violated in the \\(2N\\) sample, because a duplicating pair of data points are not independent by definition. Meanwhile, when mechanically running the linear regression on the \\(2N\\) sample (or mechanically applying the least-square formula with the twice-as-large sample), we have pretended that data points in the \\(2N\\) sample are actually i.i.d. The (spurious) standard error reduction comes from the fact that if data in the \\(2N\\) sample are truly i.i.d., then you are indeed working with additional information, which is the fact that each unique data point magically has an exact repeated observation drawn independently from the same underlying population.\n\n\nQuestion 2: Generalization Error of Linear Regression\nNext, let’s consider a numeric prediction problem and treat the linear regression as a predictive model trained on the labeled sample \\((x_i,y_i)\\). Any standard statistical learning textbook would tell us that the generalization error (i.e., expected out-of-sample \\(L_2\\) loss) of such a model can be decomposed into the sum of bias and variance:\n\\[\n\\mathbb{E}(\\widehat{Y}-Y)^2=(\\mathbb{E}(\\widehat{\\beta}) - \\beta)^2 + Var(\\widehat{\\beta}) + \\sigma^2\n\\]\nwhere \\(\\sigma^2=Var(\\varepsilon)\\). Because the regression is unbiased (free from any endogeneity issue), the generalization error simplifies to \\(Var(\\widehat{\\beta}) + \\sigma^2\\). Suppose we train two linear regression models, one on the \\(N\\) sample and another on the (duplicated) \\(2N\\) sample. Based on this generalization error expression, the model trained on \\(2N\\) sample would have smaller \\(Var(\\widehat{\\beta})\\) and therefore smaller generalization error. In other words, we are able to improve the (expected out-of-sample) predictive performance of a linear regression model simply by duplicating its training data. However, this conclusion is also preposterous - in fact, because the coefficient estimate stays the same (as we discussed in Question 1), the regression line doesn’t change at all across the two samples, and therefore would give the exact same predictions for any data point.\nBy now it should be clear what has gone wrong. The variance reduction on \\(2N\\) sample is wishful thinking. It comes from implicitly assuming the i.i.d condition when in fact it is clearly violated by the way we construct the \\(2N\\) sample. In conclusion, simply by making copies of data points in a sample has no meaningful effect on either the estimation of regression coefficients or the regression model’s predictive ability."
  },
  {
    "objectID": "posts/2023-12-26-Potential-Outcome/2023-12-26-Potential-Outcome.html",
    "href": "posts/2023-12-26-Potential-Outcome/2023-12-26-Potential-Outcome.html",
    "title": "Common Identification Strategies and Program Evaluation",
    "section": "",
    "text": "Basic Setup\nThe potential outcome framework is arguably one of the main theorecial building blocks of causal inference. In program evaluation (i.e., a term often used in econometrics to refer to the evaluation / estimation of the “effect” of certain treatment or program), people frequently talk about quantities such as ATE (average treatement effect), ATT (average treatment effect on the treated), LATE (local average treatment effect), and also connect them with identification strategies, respectively linear regression, DID regression, and IV regression. The purpose of this blog post is to clarify these connections by providing an explanation for each.\nFor basic setup, let’s index individuals (often the unit of analysis in a given estimation task) by \\(i\\), and use \\(Y_i(d)\\), where \\(d \\in \\{0,1\\}\\), to denote the potential outcomes if \\(i\\) was untreated or treated, respectively. A key idea in the potential outcome framework is that each individual, regardless of which group they are actually assigned to, fundamentally has a potential outcome for each condition. In reality, of course, only one of the potential outcomes can be observed (otherwise treatment effect estimation would have been trivial, simply \\(Y_i(1) - Y_i(0)\\)). Because of this partial observability, it makes sense to also keep track of treatment assignment with a variable \\(D_i \\in \\{0,1\\}\\). The goal of program evaluation is to estimate the effect of treatment (more formal definitions to come) using these partially observed outcomes.\n\n\nLinear Regression and ATE\nStarting with the simplest and cleanest scenario, where treatment is randomly assigned to individuals and each individual fully comply with the assigned treatment (think about a clinical trial where test pills or placebo pills are directly fed to volunteers’). Using our notations, this simply means that \\(\\forall i\\), the potential outcome \\(Y_i(D_i)\\) is observed.\nA standard approach of estimating treatment effect is via a linear regression (equivalent to a \\(t\\)-test if there is no other covariates):\n\\[\nY_i(D_i) = \\beta_0 + \\beta_{LR} D_i + \\varepsilon_i\n\\]\nThe interpretation of \\(\\beta_{LR}\\) is straightforward: \\(\\beta_{LR} = \\mathbb{E}(Y_i(D_i) \\vert D_i = 1) - \\mathbb{E}(Y_i(D_i) \\vert D_i = 0) = \\mathbb{E}(Y_i(1) - Y_i(0)).\\) This quantity is ATE by definition, which can be readily estimated by a linear regression in a randomized experiment without non-compliance.\n\n\nIV Regression and LATE\nOf course, not all settings are that clean. A common issue that arises in program evaluation is non-compliance. Think about another clinical trial where test pills or placebo pills are given to volunteers to take home. Compliers would take their intended pills and realize \\(Y_i(D_i)\\), whereas non-compliers may, for example, simply forget to take their pills (in which case \\(Y_i(0)\\) would be realized regardless of assigned conditions). Worse yet, individuals may decide to take the pills or not based on their perceived benefits, which can break the intended randomization.\nGiven the assigned treatment \\(D_i\\), the actual treatment status of individual \\(i\\) depends on that particular individual’s own choice (e.g., whether \\(i\\) decides to swallow the pill or not), which subsequently determines the realized outcome. This extra layer of uncertainty that “assigned treatment may not equal to received treatment” is why program evaluation with potential non-compliance is a mind-twister. To keep track of things clearly, let’s use \\(W_i(D_i) \\in \\{0,1\\}\\) to denote the actual treatment status of \\(i\\), and \\(Y_i(W_i(D_i))\\) to denote the realized outcome. Although the notation is a bit cumbersome, it has the advantage of clarity.\nAlthough \\(D_i\\) may be randomly assigned, \\(W_i(D_i)\\) is not, and therefore regressing \\(Y_i(W_i(D_i))\\) on \\(W_i(D_i)\\) is no longer a reliable way to estimate treatment effect. However, \\(D_i\\) naturally serves as a valid instrumental variable for \\(W_i(D_i)\\), and we can tease out a form of treatment effect estimate via 2SLS. Standard 2SLS argument gives the so-called “wald estimator”:\n\\[\n\\beta_{IV} = \\frac{\\mathbb{E}(Y_i(W_i(D_i)) | D_i = 1) - \\mathbb{E}(Y_i(W_i(D_i)) | D_i = 0)}{\\mathbb{E}(W_i(D_i) | D_i = 1) - \\mathbb{E}(W_i(D_i) | D_i = 0)}\n\\]\nBut what does this mean? To get an intuitive understanding, the following table helps.\n\n\n\n\n\\(W_i(D_i)\\)\n\\(Y_i(W_i(D_i))\\)\n\n\n\n\nComplier\n\\(D_i\\)\n\\(Y_i(D_i)\\)\n\n\nNever-Taker\n\\(0\\)\n\\(Y_i(0)\\)\n\n\nDefier\n\\(1 - D_i\\)\n\\(Y_i(1-D_i)\\)\n\n\nAlways-Taker\n\\(1\\)\n\\(Y_i(1)\\)\n\n\n\nLet’s start with the denominator: \\(\\mathbb{E}(W_i(D_i) \\vert D_i = 1)\\) is simply the proportion of individuals who actually received treatment among those who were assigned treatment. Based on the above table, it is the compliers plus the always-takers. Similarly, \\(\\mathbb{E}(W_i(D_i) \\vert D_i = 0)\\) is the proportion of individuals who would receive treatment even if they were assigned to control. It is the defiers plus the always-takers. Under the common assumption that there is no defier, the denominator reflects the proportion of compliers, i.e., individuals who received treatment only because they were assigned to the treatment group.\nBy the same logic (and with the help of the table), the numerator reflects the expected outcome change associated with compliers as \\(D_i\\) changes from 0 to 1. Therefore, the division of the two then becomes the treatment effect conditional on compliers, i.e., \\(\\mathbb{E}(Y_i(1) - Y_i(0) \\vert i \\in \\text{Complier})\\). This quantity is LATE, as it measures the treatment effect locally, for the complier group. Of course, this is not a rigorous proof, but you can find one in many econometrics textbooks / lecture materials, such as this one.\n\n\nDID Regression and ATT\nWhat if there is no non-compliance, but the treatment is not randomly assigned? In the absence of a randomized experiment, we generally cannot hope to estimate treatment effect with a linear regression of outcome on (non-random) treatment. Sometimes, however, we find ourselves in a quasi-experimental setting where the treatment manifest as a “shock” in time (e.g., introduction of some new features on a platform), and affects some individuals while others remain untreated. This two-group two-period setting is suitable for a DID regression.\nIn a typical (panel) DID setup, there is a time indicator \\(T \\in \\{0,1\\}\\) that marks “before” vs. “after” the shock, and a treatment indicator \\(D_i \\in \\{0,1\\}\\) defined the same as before. \\(Y_{i,T}(D_i)\\) therefore reflects the potential outcomes of individual in period \\(T\\) with treatment status \\(D_i\\). By convention, \\(Y_{i,0}(D_i)\\) are often called pre-treatment outcomes and \\(Y_{i,1}(D_i)\\) post-treatment outcomes.\nThe standard DID regression takes the following form (a.k.a a two-way fixed-effect regression):\n\\[\nY_{i,T}(D_i) = \\beta_0 + \\beta_1 D_i + \\beta_2 T + \\beta_{DID} D_i \\times T + \\varepsilon_i\n\\]\nwhere \\(\\beta_1 D_i\\) and \\(\\beta_2 T\\) respectively account for individual-specific and period-specific unobserved factors that may have affected treatment assignment, and \\(\\beta_{DID}\\) is the coefficient of interest.\nAgain, what does \\(\\beta_{DID}\\) measure here? As the intuition of “diff-in-diff” goes, it might seem that\n\\[\n\\beta_{DID} = [\\mathbb{E}(Y_{i,1}(1)) - \\mathbb{E}(Y_{i,0}(1))] - [\\mathbb{E}(Y_{i,1}(0)) - \\mathbb{E}(Y_{i,0}(0))]\n\\]\nHowever, this is not entirely accurate. Note that the term in the first \\([.]\\) can only be estimated among individuals in the treated group (who are affected by the treatment shock), and the term in the second \\([.]\\) can only be estimated among individuals in the control group (who are not affected by the treatment shock). So, more precisely:\n\\[\n\\beta_{DID} = [\\mathbb{E}(Y_{i,1}(1) - Y_{i,0}(1) | i \\in \\text{Treated})] - [\\mathbb{E}(Y_{i,1}(0) - Y_{i,0}(0) | i \\in \\text{Control})]\n\\]\nBut this is not very satisfactory. For each particular \\(i\\), it either contributes to the estimation of the first term or the second term, but not both. As far as “treatment estimation” goes, we ideally want to understand the effect on the \\(i\\), imagine if it was treated vs. not treated. This is where the parallel trend assumption comes in, which asserts that treated and control individuals are “similar” in the absence of treatment. Mathematically, it means\n\\[\n\\mathbb{E}(Y_{i,1}(0) - Y_{i,0}(0) | i \\in \\text{Control}) = \\mathbb{E}(Y_{i,1}(0) - Y_{i,0}(0) | i \\in \\text{Treated})\n\\]\nThis assumption says that, suppose the shock never happened (i.e., in the absence of treatment), then the cross-period change in outcome should (in expectation) be the same regardless of whether an individual was assigned to the treatment group or the control group. In other words, the shock is the only reason for any outcome divergence between treated and control individuals. In practice, this assumption is often tested by comparing the observed pre-treatemnt outcomes between treated and control individuals.\nWith this assumption, we can re-write \\(\\beta_{DID}\\) as\n\\[\n\\begin{align*}\n\\beta_{DID} & = [\\mathbb{E}(Y_{i,1}(1) - Y_{i,0}(1) | i \\in \\text{Treated})] - [\\mathbb{E}(Y_{i,1}(0) - Y_{i,0}(0) | i \\in \\text{Treated})] \\\\\n& = [\\mathbb{E}(Y_{i,1}(1) - Y_{i,1}(0) | i \\in \\text{Treated})] - [\\mathbb{E}(Y_{i,0}(1) - Y_{i,0}(0) | i \\in \\text{Treated})]\n\\end{align*}\n\\]\nThe second expectation term equals 0 because, at time period \\(T = 0\\), the treatment hasn’t taken place yet. So, in the end, we have\n\\[\n\\beta_{DID} = \\mathbb{E}(Y_{i,1}(1) - Y_{i,1}(0) | i \\in \\text{Treated})\n\\]\nwhich is referred to at ATT and reflects the average treatment effect for those that received the treatment."
  }
]